{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code plots the unmodified SMUrF and UrbanVPRM NEE over the city of Toronto for a week in July, 2018.\n",
    "# Code used to produce Fig 5 a-c of Madsen-Colford et al. \n",
    "# If used please cite\n",
    "\n",
    "# *** denotes section of the code that should be changed by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "from scipy import optimize as opt \n",
    "from scipy import odr\n",
    "import shapefile as shp # to import outline of GTA\n",
    "from shapely import geometry # used to define a polygon for Toronto\n",
    "import netCDF4\n",
    "from netCDF4 import Dataset, date2num #for reading netCDF data files and their date (not sure if I need the later)\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in unmodified UrbanVPRM data\n",
    "# *** CHANGE PATH & FILENAME ***\n",
    "VPRM_path = 'C:/Users/kitty/Documents/Research/SIF/UrbanVPRM/UrbanVPRM/dataverse_files/GTA_500m_V061_no_adjustments_2018/'\n",
    "VPRM_fn = 'vprm_mixed_GTA_500m_V061_2018_no_adjustment_' #without block number\n",
    "\n",
    "VPRM_data=pd.read_csv(VPRM_path+VPRM_fn+'00000001.csv').loc[:,('HoY','Index','GEE','Re')]\n",
    "VPRM_data2=pd.read_csv(VPRM_path+VPRM_fn+'00002501.csv').loc[:,('HoY','Index','GEE','Re')]\n",
    "VPRM_data=VPRM_data.append(VPRM_data2)\n",
    "del VPRM_data2\n",
    "\n",
    "VPRM_data2=pd.read_csv(VPRM_path+VPRM_fn+'00005001.csv').loc[:,('HoY','Index','GEE','Re')]\n",
    "VPRM_data=VPRM_data.append(VPRM_data2)\n",
    "del VPRM_data2\n",
    "\n",
    "VPRM_data2=pd.read_csv(VPRM_path+VPRM_fn+'00007501.csv').loc[:,('HoY','Index','GEE','Re')]\n",
    "VPRM_data=VPRM_data.append(VPRM_data2)\n",
    "del VPRM_data2\n",
    "\n",
    "VPRM_data2=pd.read_csv(VPRM_path+VPRM_fn+'00010001.csv').loc[:,('HoY','Index','GEE','Re')]\n",
    "VPRM_data=VPRM_data.append(VPRM_data2)\n",
    "del VPRM_data2\n",
    "\n",
    "VPRM_data2=pd.read_csv(VPRM_path+VPRM_fn+'00012501.csv').loc[:,('HoY','Index','GEE','Re')]\n",
    "VPRM_data=VPRM_data.append(VPRM_data2)\n",
    "del VPRM_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in x & y data & combine with VPRM index data\n",
    "# *** CHANGE FILENAME ***\n",
    "VPRM_EVI=pd.read_csv(VPRM_path+'adjusted_evi_lswi_interpolated_modis.csv').loc[:,('DOY','Index','x','y')]\n",
    "\n",
    "#Create a dataframe with just Index, x, & y values\n",
    "x=np.zeros(np.shape(VPRM_EVI.Index.unique()))*np.nan\n",
    "y=np.zeros(np.shape(VPRM_EVI.Index.unique()))*np.nan\n",
    "for i in range(len(VPRM_EVI.Index.unique())):\n",
    "    x[i]=VPRM_EVI.x[0+i*365]\n",
    "    y[i]=VPRM_EVI.y[0+i*365]\n",
    "    \n",
    "VPRM_xy=pd.DataFrame({'Index':VPRM_EVI.Index.unique(), 'x':x, 'y':y})\n",
    "VPRM_data=VPRM_data.merge(VPRM_xy[['Index','x','y']])\n",
    "del VPRM_EVI, VPRM_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the x & y values\n",
    "xvals = VPRM_data.x[VPRM_data.HoY==4800].unique()\n",
    "yvals = VPRM_data.y[VPRM_data.HoY==4800].unique()\n",
    "extent = np.min(xvals), np.max(xvals), np.min(yvals), np.max(yvals)\n",
    "\n",
    "# Reshape the GPP and Reco data\n",
    "GPP=VPRM_data.GEE.values.reshape(len(yvals),len(xvals),8760)#8784 for leap year\n",
    "Reco=VPRM_data.Re.values.reshape(len(yvals),len(xvals),8760)\n",
    "\n",
    "# Extract the hour and day of the year\n",
    "HoY=VPRM_data.HoY.values.reshape(len(yvals),len(xvals),8760)\n",
    "DoY=np.mean(HoY,axis=(0,1))/24+23/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Swap the aces to match the format of SMUrF & calculate NEE\n",
    "VPRM_GPP=(np.swapaxes(np.swapaxes(GPP,0,2),1,2))\n",
    "VPRM_Reco=(np.swapaxes(np.swapaxes(Reco,0,2),1,2))\n",
    "VPRM_NEE=(np.swapaxes(np.swapaxes(Reco,0,2),1,2)-np.swapaxes(np.swapaxes(GPP,0,2),1,2))\n",
    "\n",
    "#Take the 8-day average to match the temporal resolution of SMUrF\n",
    "VPRM_GPP_8day=np.ones((46, 96, 144))*np.nan\n",
    "VPRM_Reco_8day=np.ones((46, 96, 144))*np.nan\n",
    "VPRM_NEE_8day=np.ones((46, 96, 144))*np.nan\n",
    "for i in range(46):\n",
    "    VPRM_GPP_8day[i]=np.nanmean(VPRM_GPP[i*8*24:i*8*24+8*24],axis=0)\n",
    "    VPRM_Reco_8day[i]=np.nanmean(VPRM_Reco[i*8*24:i*8*24+8*24],axis=0)\n",
    "    VPRM_NEE_8day[i]=np.nanmean(VPRM_NEE[i*8*24:i*8*24+8*24],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the shape file for Toronto's boundary\n",
    "# *** CHANGE PATH & FILENAME ***\n",
    "sf = shp.Reader(\"C:/Users/kitty/Documents/Research/SIF/Shape_files/Toronto/Toronto_Boundary.shp\")\n",
    "shape=sf.shape(0)\n",
    "Toronto_x = np.zeros((len(shape.points),1))*np.nan\n",
    "Toronto_y = np.zeros((len(shape.points),1))*np.nan\n",
    "for i in range(len(shape.points)):\n",
    "    Toronto_x[i]=shape.points[i][0]\n",
    "    Toronto_y[i]=shape.points[i][1]\n",
    "    \n",
    "points=[]\n",
    "for k in range(1,len(Toronto_x)):\n",
    "    points.append(geometry.Point(Toronto_x[k],Toronto_y[k]))\n",
    "poly=geometry.Polygon([[p.x, p.y] for p in points])\n",
    "\n",
    "#Create a mask for areas outside the GTA\n",
    "lons=np.ones(144)*np.nan\n",
    "lats=np.ones(96)*np.nan\n",
    "GPP_mask=np.ones([96,144])*np.nan\n",
    "for i in range(0, len(lons)):\n",
    "    for j in range(0, len(lats)):\n",
    "        if poly.contains(geometry.Point([xvals[i],yvals[j]])):\n",
    "            lons[i]=xvals[i]\n",
    "            lats[j]=yvals[j]\n",
    "            GPP_mask[j,i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now bring in the original SMUrF data with V061 MODIS\n",
    "\n",
    "# *** CHANGE PATH & FILENAME ***\n",
    "SMUrF_path = 'E:/Research/SMUrF/output2018_CSIF_V061/easternCONUS/'\n",
    "SMUrF_fn = 'daily_mean_Reco_neuralnet/era5/2018/daily_mean_Reco_uncert_easternCONUS_2018' # WITHOUT the month & day (added in loop below)\n",
    "\n",
    "#load the first file to get the start of the year\n",
    "g=Dataset(SMUrF_path+SMUrF_fn+'0101.nc')\n",
    "start_of_year=g.variables['time'][0]/3600/24-1 #convert seconds since 1970 to days (minus one)\n",
    "g.close()\n",
    "\n",
    "#With ISA adjustment using GMIS-Toronto-SOLRIS-ACI dataset\n",
    "S_time=[]\n",
    "S_Reco=[]\n",
    "S_Reco_err=[]\n",
    "S_lats=[]\n",
    "S_lons=[]\n",
    "for j in range(1,13):\n",
    "    for i in range(1,32):\n",
    "        try:\n",
    "            if j<10:\n",
    "                if i<10:\n",
    "                    f=Dataset(SMUrF_path+SMUrF_fn+'0'+str(j)+'0'+str(i)+'.nc')\n",
    "                else:\n",
    "                    f=Dataset(SMUrF_path+SMUrF_fn+'0'+str(j)+str(i)+'.nc')\n",
    "            else:\n",
    "                if i<10:\n",
    "                    f=Dataset(SMUrF_path+SMUrF_fn+str(j)+'0'+str(i)+'.nc')\n",
    "                else:\n",
    "                    f=Dataset(SMUrF_path+SMUrF_fn+str(j)+str(i)+'.nc')\n",
    "            if len(S_time)==0:\n",
    "                S_lats=f.variables['lat'][:]\n",
    "                S_lons=f.variables['lon'][:]\n",
    "                S_Reco=f.variables['Reco_mean'][:]\n",
    "                S_Reco_err=f.variables['Reco_sd'][:]\n",
    "                S_time=f.variables['time'][:]/24/3600-start_of_year-5/24 #convert seconds since 1970 to days and subtract start of year\n",
    "            else:\n",
    "                S_Reco=np.concatenate((S_Reco,f.variables['Reco_mean'][:]),axis=0)\n",
    "                S_Reco_err=np.concatenate((S_Reco_err,f.variables['Reco_sd'][:]),axis=0)\n",
    "                S_time=np.concatenate((S_time,(f.variables['time'][:]/24/3600-start_of_year-5/24)),axis=0)\n",
    "            f.close()\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        \n",
    "#Load in GPP data\n",
    "f=Dataset(SMUrF_path+'daily_mean_SIF_GPP_uncert_easternCONUS_2018.nc')\n",
    "S_time=f.variables['time'][:]/24/3600-start_of_year-5/24 #convert seconds since 1970 to days and subtract start of year\n",
    "\n",
    "S_GPP_err=f.variables['GPP_sd'][:]\n",
    "S_GPP=f.variables['GPP_mean'][:]\n",
    "\n",
    "S_Reco[S_Reco==-999]=np.nan\n",
    "S_Reco_err[S_Reco_err==-999]=np.nan\n",
    "S_GPP[S_GPP==-999]=np.nan\n",
    "S_GPP_err[S_GPP_err==-999]=np.nan\n",
    "\n",
    "#Take 4-day average of Reco to match temporal resolution of GPP\n",
    "S_Reco_4d=np.ones(np.shape(S_GPP))*np.nan\n",
    "S_Reco_4d_err=np.ones(np.shape(S_GPP))*np.nan\n",
    "#S_Reco_4d_std=np.ones(np.shape(S_GPP))*np.nan\n",
    "for i in range(len(S_time)):\n",
    "    S_Reco_4d[i]=np.nanmean(S_Reco[i*4:i*4+4],axis=0)\n",
    "    S_Reco_4d_err[i]=np.sqrt(np.nansum((S_Reco_err[i*4:i*4+4]/4)**2,axis=0))\n",
    "    #S_Reco_4d_std[i]=np.nanstd(S_Reco[i*4:i*4+4],axis=0)\n",
    "    \n",
    "S_NEE=S_Reco_4d-S_GPP\n",
    "S_NEE_err=np.sqrt(S_Reco_4d_err**2+S_GPP_err**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a mask for areas outside Toronto for SMUrF\n",
    "# Also create weights for pixels that fall partially inside the boundary\n",
    "S_mask_lons=np.ones(14)*np.nan\n",
    "S_mask_lats=np.ones(10)*np.nan\n",
    "S_GPP_mask=np.ones([10,14])*np.nan\n",
    "S_mask_weight=np.ones([10,14])*np.nan\n",
    "for i in range(0, 14):\n",
    "    for j in range(0, 10):\n",
    "        pts=[geometry.Point(S_lons[19:33][i]-0.025,S_lats[19:29][j]-0.025),geometry.Point(S_lons[19:33][i]+0.025,S_lats[19:29][j]-0.025),geometry.Point(S_lons[19:33][i]+0.025,S_lats[19:29][j]+0.025),geometry.Point(S_lons[19:33][i]-0.025,S_lats[19:29][j]+0.025),geometry.Point(S_lons[19:33][i]-0.025,S_lats[19:29][j]-0.025)]\n",
    "        pixel=geometry.Polygon([[p.x, p.y] for p in pts])\n",
    "        footprint=pixel.area\n",
    "        intersect=pixel.intersection(poly)\n",
    "        \n",
    "        if intersect.area >0:\n",
    "            S_mask_lons[i]=S_lons[19:33][i]\n",
    "            S_mask_lats[j]=S_lats[19:29][j]\n",
    "            S_GPP_mask[j,i]=1\n",
    "            S_mask_weight[j,i]=intersect.area/footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take 8-day average to match temporal resolution of updated SMUrF\n",
    "S_NEE_8day=np.zeros([len(VPRM_GPP_8day),len(S_NEE[0]),len(S_NEE[0,0])])*np.nan\n",
    "S_GPP_8day=np.zeros([len(VPRM_GPP_8day),len(S_NEE[0]),len(S_NEE[0,0])])*np.nan\n",
    "S_Reco_8day=np.zeros([len(VPRM_GPP_8day),len(S_NEE[0]),len(S_NEE[0,0])])*np.nan\n",
    "S_time_8day=np.zeros(len(VPRM_GPP_8day))*np.nan\n",
    "\n",
    "for i in range(len(VPRM_NEE_8day)):\n",
    "    S_NEE_8day[i]=np.nanmean(S_NEE[i*2:i*2+2],axis=0)\n",
    "    S_GPP_8day[i]=np.nanmean(S_GPP[i*2:i*2+2],axis=0)\n",
    "    S_Reco_8day[i]=np.nanmean(S_Reco_4d[i*2:i*2+2],axis=0)\n",
    "    S_time_8day[i] = S_time[i*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the summer data\n",
    "\n",
    "#JJA: Doy 60 - 151 inclusive\n",
    "VPRM_GPP_JJA_8day=VPRM_GPP_8day[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)]\n",
    "VPRM_Reco_JJA_8day=VPRM_Reco_8day[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)]\n",
    "VPRM_NEE_JJA_8day=VPRM_NEE_8day[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)]\n",
    "\n",
    "S_JJA_time=S_time_8day[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)]\n",
    "S_GPP_JJA=S_GPP_8day[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)]\n",
    "S_Reco_JJA=S_Reco_8day[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)]\n",
    "S_NEE_JJA=S_NEE_8day[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample VPRM data at SMUrF resolution\n",
    "\n",
    "# Convert array to an xarray 'Data Array'\n",
    "VPRM_doy_185= VPRM_NEE_JJA_8day[4,:,12:]\n",
    "VPRM_NEE_185_da = xr.DataArray(VPRM_doy_185,coords=[yvals,xvals[12:]])\n",
    "VPRM_NEE_185_da = VPRM_NEE_185_da.rename({'dim_0':'lat','dim_1':'lon'})\n",
    "\n",
    "# Convert the NEE to a dataset\n",
    "VPRM_NEE_185_ds = VPRM_NEE_185_da.to_dataset(name='VPRM_NEE')\n",
    "#Resample the dataset to SMUrF's resolution\n",
    "VPRM_NEE_185_resamp_ds = VPRM_NEE_185_ds.coarsen(lon=12).mean().coarsen(lat=12).mean()\n",
    "\n",
    "# Convert it back to a Data Array\n",
    "VPRM_NEE_185_resamp_da = VPRM_NEE_185_resamp_ds.to_array()\n",
    "VPRM_NEE_185_resamp_da = VPRM_NEE_185_resamp_da.drop_vars('variable')\n",
    "VPRM_NEE_185_resamp_da = VPRM_NEE_185_resamp_da[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a mask for the resampled UrbanVPRM for areas outside Toronto\n",
    "VPRM_resamp_mask_lons=np.ones(11)*np.nan\n",
    "VPRM_resamp_mask_lats=np.ones(8)*np.nan\n",
    "VPRM_resamp_GPP_mask=np.ones([8,11])*np.nan\n",
    "VPRM_resamp_mask_weight=np.ones([8,11])*np.nan\n",
    "for i in range(0, 11):\n",
    "    for j in range(0, 8):\n",
    "        pts=[geometry.Point(VPRM_NEE_185_resamp_da['lon'][i]-0.025,VPRM_NEE_185_resamp_da['lat'][j]-0.025),geometry.Point(VPRM_NEE_185_resamp_da['lon'][i]+0.025,VPRM_NEE_185_resamp_da['lat'][j]-0.025),geometry.Point(VPRM_NEE_185_resamp_da['lon'][i]+0.025,VPRM_NEE_185_resamp_da['lat'][j]+0.025),geometry.Point(VPRM_NEE_185_resamp_da['lon'][i]-0.025,VPRM_NEE_185_resamp_da['lat'][j]+0.025),geometry.Point(VPRM_NEE_185_resamp_da['lon'][i]-0.025,VPRM_NEE_185_resamp_da['lat'][j]-0.025)]\n",
    "        VPRM_pixel=geometry.Polygon([[p.x, p.y] for p in pts])\n",
    "        footprint=VPRM_pixel.area\n",
    "        intersect=VPRM_pixel.intersection(poly)\n",
    "        \n",
    "        if intersect.area >0:\n",
    "            VPRM_resamp_mask_lons[i]=VPRM_NEE_185_resamp_da['lon'][i]\n",
    "            VPRM_resamp_mask_lats[j]=VPRM_NEE_185_resamp_da['lat'][j]\n",
    "            VPRM_resamp_GPP_mask[j,i]=1\n",
    "            VPRM_resamp_mask_weight[j,i]=intersect.area/footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an extent for the resampled UrbanVPRM\n",
    "VPRM_resamp_extent=np.min(S_lons[21:33])-0.025, np.max(S_lons[21:33])-0.025, np.min(S_lats[20:29])-0.025, np.max(S_lats[20:29])-0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crop the SMUrF data to Toronto\n",
    "S_lons_cropped=S_lons[21:33]-0.025\n",
    "S_lats_cropped=S_lats[20:29]-0.025\n",
    "S_NEE_8day_cropped=S_NEE_8day[23,20:28,21:32]\n",
    "S_GPP_mask_cropped=S_GPP_mask[1:9,2:13]\n",
    "S_mask_weight_cropped=S_mask_weight[1:9,2:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an extent for the cropped SMUrF data\n",
    "S_cropped_extent=np.min(S_lons_cropped), np.max(S_lons_cropped), np.min(S_lats_cropped), np.max(S_lats_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original SMUrF and UrbanVPRM and their difference for one week in July (fig 5 a-c)\n",
    "\n",
    "plt.rc('font',size=21.5)\n",
    "fig, ax = plt.subplots(1,3,sharex=True,sharey=True,figsize=(24.5,6))\n",
    "ax[0].set_xlim(-79.69,-79.06)\n",
    "ax[0].set_ylim(43.5,43.9)\n",
    "\n",
    "\n",
    "fig0=ax[0].pcolormesh(S_lons_cropped,S_lats_cropped,S_NEE_8day_cropped*S_GPP_mask_cropped,cmap='bwr',vmin=-10,vmax=10)\n",
    "ax[0].plot(Toronto_x,Toronto_y,c='k')\n",
    "ax[0].text(-79.66,43.51, 'Average NEE = '+str(np.round(np.nansum(S_NEE_8day_cropped*S_mask_weight_cropped)/np.nansum(S_mask_weight_cropped),3))+' $\\mu$mol m$^{-2}$ s$^{-1}$',fontsize=24)\n",
    "ax[0].set_title('Unmodified SMUrF Toronto NEE',fontsize=25)\n",
    "    \n",
    "fig1=ax[1].pcolormesh(xvals,yvals,VPRM_NEE_JJA_8day[4]*GPP_mask,cmap='bwr',vmin=-10,vmax=10)\n",
    "ax[1].plot(Toronto_x,Toronto_y,c='k')\n",
    "#Using average at 0.05 res (with weights depending on how much of each pixel falls within city limits):\n",
    "ax[1].text(-79.675,43.51, 'Average NEE = '+str(np.round(np.nansum(VPRM_NEE_185_resamp_da*VPRM_resamp_mask_weight)/(np.nansum(VPRM_resamp_mask_weight)),3))+' $\\mu$mol m$^{-2}$ s$^{-1}$',fontsize=24)\n",
    "ax[1].set_title('Unmodified UrbanVPRM Toronto NEE', fontsize=25)\n",
    "\n",
    "fig2=ax[2].pcolormesh(S_lons_cropped,S_lats_cropped,S_NEE_8day_cropped*S_GPP_mask_cropped-(VPRM_NEE_185_resamp_da[::-1])*S_GPP_mask_cropped,cmap='bwr',vmin=-10,vmax=10)\n",
    "ax[2].plot(Toronto_x,Toronto_y,c='k')\n",
    "ax[2].text(-79.68,43.51, 'Average $\\Delta$NEE = '+str(np.round(np.nansum(S_NEE_8day_cropped*S_mask_weight_cropped-VPRM_NEE_185_resamp_da[::-1]*S_mask_weight_cropped)/np.nansum(S_mask_weight_cropped),3))+' $\\mu$mol m$^{-2}$ s$^{-1}$',fontsize=24)\n",
    "ax[2].set_title('SMUrF - UrbanVPRM Toronto NEE',fontsize=25)\n",
    "\n",
    "cbar_ax = fig.add_axes([0.91, 0.15, 0.02, 0.7])\n",
    "cbar=fig.colorbar(fig1,cax=cbar_ax)\n",
    "cbar.set_label('NEE ($\\mu$mol m$^{-2}$ s$^{-1}$)',fontsize=24)\n",
    "\n",
    "ax[0].text(-79.68,43.865,'(a)',c='k',fontsize=26)\n",
    "ax[1].text(-79.68,43.865,'(b)',c='k',fontsize=26)\n",
    "ax[2].text(-79.68,43.865,'(c)',c='k',fontsize=26)\n",
    "\n",
    "ax[0].set_ylabel('Latitude ($^o$)',fontsize=24)\n",
    "\n",
    "ax[0].set_xlabel('Longitude ($^o$)',fontsize=24)\n",
    "ax[1].set_xlabel('Longitude ($^o$)',fontsize=24)\n",
    "ax[2].set_xlabel('Longitude ($^o$)',fontsize=24)\n",
    "\n",
    "fig.subplots_adjust(hspace=0,wspace=0)\n",
    "# *** Uncomment to save figure as png and pdf. CHANGE FILENAME ***\n",
    "#plt.savefig('Original_SMUrF_vs_V061_UrbanVPRM_NEE_aggregated_avg_unmodified_clim_10_larger_font_DoY_'+str(int(round(S_JJA_time[4])))+'_fixed_labelled.pdf',bbox_inches='tight')\n",
    "#plt.savefig('Original_SMUrF_vs_V061_UrbanVPRM_NEE_aggregated_avg_unmodified_clim_10_larger_font_DoY_'+str(int(round(S_JJA_time[4])))+'_fixed_labelled.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With minimum R value in UrbanVPRM\n",
    "#V061 MODIS\n",
    "\n",
    "VPRM_avg=np.zeros(len(S_NEE_8day))*np.nan\n",
    "SMUrF_avg=np.zeros(len(S_NEE_8day))*np.nan\n",
    "SMAPE=np.zeros(len(S_NEE_8day))*np.nan\n",
    "diff=np.zeros(len(S_NEE_8day))*np.nan\n",
    "diff_SMUrF=np.zeros(len(S_NEE_8day))*np.nan\n",
    "\n",
    "for i in range(len(S_NEE_8day)):\n",
    "    S_NEE_8day_cropped_i=S_NEE_8day[i,20:28,21:32]\n",
    "    VPRM_doy_i= VPRM_NEE_8day[i,:,12:]\n",
    "    VPRM_NEE_i_da = xr.DataArray(VPRM_doy_i,coords=[yvals-1/240/2,xvals[12:]-1/240/2])\n",
    "    VPRM_NEE_i_da = VPRM_NEE_i_da.rename({'dim_0':'lat','dim_1':'lon'})\n",
    "\n",
    "    VPRM_NEE_i_ds = VPRM_NEE_i_da.to_dataset(name='VPRM_NEE')\n",
    "\n",
    "    VPRM_NEE_i_resamp_ds = VPRM_NEE_i_ds.coarsen(lon=12).mean().coarsen(lat=12).mean()\n",
    "\n",
    "    VPRM_NEE_i_resamp_da = VPRM_NEE_i_resamp_ds.to_array()\n",
    "\n",
    "    VPRM_NEE_i_resamp_da = VPRM_NEE_i_resamp_da.drop_vars('variable')\n",
    "    VPRM_NEE_i_resamp_da = VPRM_NEE_i_resamp_da[0]\n",
    "    \n",
    "    VPRM_avg[i]=(np.nansum(VPRM_NEE_i_resamp_da[::-1]*S_mask_weight_cropped)/np.nansum(S_mask_weight_cropped))\n",
    "    SMUrF_avg[i] = (np.nansum(S_NEE_8day_cropped_i*S_mask_weight_cropped)/np.nansum(S_mask_weight_cropped))\n",
    "    SMAPE[i]=abs(SMUrF_avg[i]-VPRM_avg[i])/((abs(SMUrF_avg[i])+abs(VPRM_avg[i]))/2)\n",
    "    diff[i]=(SMUrF_avg[i]-VPRM_avg[i])/((abs(SMUrF_avg[i])+abs(VPRM_avg[i]))/2)\n",
    "    diff_SMUrF[i]=(SMUrF_avg[i]-VPRM_avg[i])/((abs(SMUrF_avg[i]))/2)\n",
    "    \n",
    "    #Uncomment to print the Percent Difference for each 8-day average over Toronto:\n",
    "    #print(\"Doy \"+str(i*8+1)+' |SMUrF-VPRM|/(|SMUrF|+|VPRM|/2) NEE: '+str(np.round(SMAPE[i],3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *** Uncomment to print Summer SMUrF averages over Toronto\n",
    "#print('Toronto SMUrF 8-day Summer Averages: '+\n",
    "#      str(np.round(SMUrF_avg[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)],5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *** Uncoment to print Summer UrbanVPRM averages over Toronto\n",
    "#print('Toronto UrbanVPRM 8-day Summer Averages: '+\n",
    "#      str(np.round(VPRM_avg[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)],5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *** Uncoment to print mean Summer SMUrF averages over Toronto\n",
    "print('JJA mean SMUrF = '+str(np.round(np.mean(SMUrF_avg[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)]),2))+' +/- '+str(np.round(np.std(SMUrF_avg[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)]),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *** Uncoment to print mean Summer UrbanVPRM averages over Toronto\n",
    "print('JJA mean VPRM = '+str(np.round(np.mean(VPRM_avg[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)]),2))+' +/- '+str(np.round(np.std(VPRM_avg[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)]),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('JJA mean Percent difference = '+str(np.round(np.mean(SMAPE[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)])*100,2))+' +/- '+str(np.round(np.std(SMAPE[(np.round(S_time_8day,5)>=152) & (np.round(S_time_8day,5)<244)])*100,2))+' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
