{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code takes NEE, GPP and Reco from the updated UrbanVPRM and compares them to that of the updated SMUrF \n",
    "# both with and without the impervious surface area (ISA) adjustment over the City of Toronto. \n",
    "# For each hour of the year a bootstrapped Huber fit is applied between SMUrF and UrbanVPRM for all pixels that fall\n",
    "# inside the bounds of the City of Toronto to calculate the correlation coefficient (R^2) for each hour of the year.\n",
    "# The Root Mean Square Error (RMSE) and Mean Bias Error (MBE) are also calculated for each hour.\n",
    "# The results are plotted as time series for each of NEE, GPP, and Reco.\n",
    "\n",
    "# This code reproduces figure 4 of Madsen-Colford et al. 2025\n",
    "# If used please cite\n",
    "\n",
    "# *** denote portions of the code that should be modified by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "from scipy import optimize as opt \n",
    "from scipy import odr\n",
    "import shapefile as shp # to import outline of GTA\n",
    "from shapely import geometry # used to define a polygon for Toronto\n",
    "import netCDF4\n",
    "from netCDF4 import Dataset, date2num #for reading netCDF data files and their date (not sure if I need the later)\n",
    "from sklearn import linear_model #for doing robust fits\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.colors as clrs #for log color scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in VPRM data\n",
    "# *** CHANGE PATH *** \n",
    "VPRM_path = 'E:/Research/UrbanVPRM/dataverse_files/GTA_V061_500m_2018/'\n",
    "# *** CHANGE FILE NAME ***\n",
    "VPRM_fn = 'vprm_GMIS_Toronto_ACI_SOLRIS_ISA_500m_GTA_V061_2018_no_PScale_adjusted_Topt_Ra_URB_parameters_fixed_gapfilled_LSWI_filtered_bilinear_PAR_block_'\n",
    "\n",
    "VPRM_data = pd.read_csv(VPRM_path+VPRM_fn+'00000001.csv').loc[:,('HoY','Index','GEE','Re')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VPRM_data2=pd.read_csv(VPRM_path+VPRM_fn+'00002501.csv').loc[:,('HoY','Index','GEE','Re')]\n",
    "VPRM_data=VPRM_data.append(VPRM_data2)\n",
    "del VPRM_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VPRM_data2=pd.read_csv(VPRM_path+VPRM_fn+'00005001.csv').loc[:,('HoY','Index','GEE','Re')]\n",
    "VPRM_data=VPRM_data.append(VPRM_data2)\n",
    "del VPRM_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VPRM_data2=pd.read_csv(VPRM_path+VPRM_fn+'00007501.csv').loc[:,('HoY','Index','GEE','Re')]\n",
    "VPRM_data=VPRM_data.append(VPRM_data2)\n",
    "del VPRM_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VPRM_data2=pd.read_csv(VPRM_path+VPRM_fn+'00010001.csv').loc[:,('HoY','Index','GEE','Re')]\n",
    "VPRM_data=VPRM_data.append(VPRM_data2)\n",
    "del VPRM_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VPRM_data2=pd.read_csv(VPRM_path+VPRM_fn+'00012501.csv').loc[:,('HoY','Index','GEE','Re')]\n",
    "VPRM_data=VPRM_data.append(VPRM_data2)\n",
    "del VPRM_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** CHANGE PATH & FILE NAME ***\n",
    "VPRM_EVI=pd.read_csv('E:/Research/UrbanVPRM/dataverse_files/GTA_V061_500m_2018/adjusted_evi_lswi_interpolated_modis_v061_qc_filtered_LSWI_filtered.csv').loc[:,('DOY','Index','x','y')]\n",
    "\n",
    "#Create a dataframe with just Index, x, & y values\n",
    "x=np.zeros(np.shape(VPRM_EVI.Index.unique()))*np.nan\n",
    "y=np.zeros(np.shape(VPRM_EVI.Index.unique()))*np.nan\n",
    "for i in range(len(VPRM_EVI.Index.unique())):\n",
    "    x[i]=VPRM_EVI.x[0+i*365]\n",
    "    y[i]=VPRM_EVI.y[0+i*365]\n",
    "\n",
    "# Make a dataframe of index, x, & y and merge that with the VPRM data based on the index\n",
    "VPRM_xy=pd.DataFrame({'Index':VPRM_EVI.Index.unique(), 'x':x, 'y':y})\n",
    "VPRM_data=VPRM_data.merge(VPRM_xy[['Index','x','y']])\n",
    "del VPRM_EVI, VPRM_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the unique x and y values & define an extent\n",
    "xvals = VPRM_data.x[VPRM_data.HoY==4800].unique()\n",
    "yvals = VPRM_data.y[VPRM_data.HoY==4800].unique()\n",
    "extent = np.min(xvals), np.max(xvals), np.min(yvals), np.max(yvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the GPP and Reco data into an array\n",
    "GPP=-VPRM_data.GEE.values.reshape(len(yvals),len(xvals),8760)#8784 for leap year\n",
    "Reco=VPRM_data.Re.values.reshape(len(yvals),len(xvals),8760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in a shape file for Toronto, used to select data inside the city\n",
    "\n",
    "# *** CHANGE PATH AND FILE NAME ***\n",
    "sf = shp.Reader(\"C:/Users/kitty/Documents/Research/SIF/Shape_files/Toronto/Toronto_Boundary.shp\")\n",
    "#Toronto_Shape\n",
    "shape=sf.shape(0)\n",
    "#Need to partition each individual shape\n",
    "Toronto_x = np.zeros((len(shape.points),1))*np.nan #make arrays of x & y values for the outline\n",
    "Toronto_y = np.zeros((len(shape.points),1))*np.nan\n",
    "for i in range(len(shape.points)):\n",
    "    Toronto_x[i]=shape.points[i][0]\n",
    "    Toronto_y[i]=shape.points[i][1]\n",
    "    \n",
    "points=[]\n",
    "for k in range(1,len(Toronto_x)): #convert x & y data to points\n",
    "    points.append(geometry.Point(Toronto_x[k],Toronto_y[k]))\n",
    "poly=geometry.Polygon([[p.x, p.y] for p in points]) #convert points to a polygon\n",
    "\n",
    "#Create a mask for areas outside the city of Toronto\n",
    "lons=np.ones(144)*np.nan\n",
    "lats=np.ones(96)*np.nan\n",
    "GPP_mask=np.ones([96,144])*np.nan\n",
    "for i in range(0, len(lons)):\n",
    "    for j in range(0, len(lats)):\n",
    "        if poly.contains(geometry.Point([xvals[i],yvals[j]])):\n",
    "            lons[i]=xvals[i]\n",
    "            lats[j]=yvals[j]\n",
    "            GPP_mask[j,i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a mask so that only areas within the city of Toronto are given\n",
    "T_VPRM_GPP=GPP*GPP_mask[:,:,np.newaxis]\n",
    "T_VPRM_Reco=Reco*GPP_mask[:,:,np.newaxis]\n",
    "T_VPRM_NEE=Reco-GPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now bring in the SMUrF data with ISA adjustment, shoreline correction, AND downscaling fix\n",
    "\n",
    "# Bring in the first Reco file and extract the first day of the year (in seconds since 1970)\n",
    "# ***CHANGE PATH AND FILE NAME ***\n",
    "g=Dataset('C:/Users/kitty/Documents/Research/SIF/SMUrF/output2018_500m_CSIF_to_TROPOMI_CSIF_ALL_converted_slps_V3_temp_impervious_R_shore_corr_V061_8day/easternCONUS/daily_mean_Reco_ISA_a_neuralnet/era5/2018/daily_mean_Reco_uncert_GMIS_Toronto_t_easternCONUS_20180101.nc')\n",
    "start_of_year=g.variables['time'][0]/3600/24-1 #convert seconds since 1970 to days (minus one)\n",
    "g.close()\n",
    "\n",
    "\n",
    "#Load in SMUrF NEE data\n",
    "# *** CHANGE PATH ***\n",
    "SMUrF_path = 'C:/Users/kitty/Documents/Research/SIF/SMUrF/output2018_500m_CSIF_to_TROPOMI_CSIF_ALL_converted_slps_V3_temp_impervious_R_shore_corr_V061_8day/easternCONUS/hourly_flux_GMIS_combined_ISA_a_w_sd_era5/'\n",
    "# *** CHANGE FILE NAME ***\n",
    "SMUrF_fn = 'hrly_mean_GPP_Reco_NEE_easternCONUS_2018'\n",
    "\n",
    "S_time=[]\n",
    "S_Reco=[]\n",
    "S_NEE=[]\n",
    "S_GPP=[]\n",
    "S_lats=[]\n",
    "S_lons=[]\n",
    "for j in range(1,13): # *** ADJUST THIS TO USE SPECIFIC MONTHS ***\n",
    "    try:\n",
    "        if j<10:\n",
    "            f=Dataset(SMUrF_path+SMUrF_fn+'0'+str(j)+'.nc')\n",
    "        else:\n",
    "            f=Dataset(SMUrF_path+SMUrF_fn+str(j)+'.nc')\n",
    "        if len(S_time)==0:\n",
    "            S_lats=f.variables['lat'][:]\n",
    "            S_lons=f.variables['lon'][:]\n",
    "            S_Reco=f.variables['Reco_mean'][:,264:360,288:432]\n",
    "            S_GPP=f.variables['GPP_mean'][:,264:360,288:432]\n",
    "            S_NEE=f.variables['NEE_mean'][:,264:360,288:432]\n",
    "            S_time=f.variables['time'][:]/24/3600-start_of_year-5/24 #convert seconds since 1970 to days and subtract start of year\n",
    "        else:\n",
    "            S_Reco=np.concatenate((S_Reco,f.variables['Reco_mean'][:,264:360,288:432]),axis=0)\n",
    "            S_GPP=np.concatenate((S_GPP,f.variables['GPP_mean'][:,264:360,288:432]),axis=0)\n",
    "            S_NEE=np.concatenate((S_NEE,f.variables['NEE_mean'][:,264:360,288:432]),axis=0)\n",
    "            S_time=np.concatenate((S_time,(f.variables['time'][:]/24/3600-start_of_year-5/24)),axis=0)\n",
    "        f.close()\n",
    "    except FileNotFoundError:\n",
    "        print(j)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove fill values and replace with NAN\n",
    "S_Reco[S_Reco==-999]=np.nan\n",
    "S_NEE[S_NEE==-999]=np.nan\n",
    "S_GPP[S_GPP==-999]=np.nan\n",
    "\n",
    "# Apply Toronto mask to SMUrF data\n",
    "T_S_GPP=S_GPP[:,::-1]*GPP_mask[np.newaxis,:,:]\n",
    "T_S_Reco=S_Reco[:,::-1]*GPP_mask[np.newaxis,:,:]\n",
    "T_S_NEE=S_NEE[:,::-1]*GPP_mask[np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now bring in SMUrF data without ISA correction\n",
    "\n",
    "# *** CHANGE PATH ***\n",
    "SMUrF_noISA_path = 'E:/Research/SMUrF/output2018_500m_CSIF_to_TROPOMI_CSIF_ALL_converted_slps_V3_temp_impervious_R_shore_corr_V061_8day/easternCONUS/hourly_flux_GMIS_Toronto_fixed_border_no_ISA_era5/'\n",
    "\n",
    "S_noISA_time=[]\n",
    "S_noISA_Reco=[]\n",
    "S_noISA_NEE=[]\n",
    "S_noISA_GPP=[]\n",
    "S_noISA_lats=[]\n",
    "S_noISA_lons=[]\n",
    "for j in range(1,13): # *** ADJUST THIS TO USE SPECIFIC MONTHS ***\n",
    "    try:\n",
    "        if j<10:\n",
    "            f=Dataset(SMUrF_noISA_path+SMUrF_fn+'0'+str(j)+'.nc')\n",
    "        else:\n",
    "            f=Dataset(SMUrF_noISA_path+SMUrF_fn+str(j)+'.nc')\n",
    "        if len(S_noISA_time)==0:\n",
    "            S_noISA_lats=f.variables['lat'][:]\n",
    "            S_noISA_lons=f.variables['lon'][:]\n",
    "            S_noISA_Reco=f.variables['Reco_mean'][:,264:360,288:432]\n",
    "            S_noISA_GPP=f.variables['GPP_mean'][:,264:360,288:432]\n",
    "            S_noISA_NEE=f.variables['NEE_mean'][:,264:360,288:432]\n",
    "            S_noISA_time=f.variables['time'][:]/24/3600-start_of_year-5/24 #convert seconds since 1970 to days and subtract start of year\n",
    "        else:\n",
    "            S_noISA_Reco=np.concatenate((S_noISA_Reco,f.variables['Reco_mean'][:,264:360,288:432]),axis=0)\n",
    "            S_noISA_GPP=np.concatenate((S_noISA_GPP,f.variables['GPP_mean'][:,264:360,288:432]),axis=0)\n",
    "            S_noISA_NEE=np.concatenate((S_noISA_NEE,f.variables['NEE_mean'][:,264:360,288:432]),axis=0)\n",
    "            S_noISA_time=np.concatenate((S_noISA_time,(f.variables['time'][:]/24/3600-start_of_year-5/24)),axis=0)\n",
    "        f.close()\n",
    "    except FileNotFoundError:\n",
    "        print(j)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace fill values with NAN\n",
    "S_noISA_Reco[S_noISA_Reco==-999]=np.nan\n",
    "S_noISA_NEE[S_noISA_NEE==-999]=np.nan\n",
    "S_noISA_GPP[S_noISA_GPP==-999]=np.nan\n",
    "\n",
    "# Apply Toronto mask\n",
    "T_S_noISA_GPP=S_noISA_GPP[:,::-1]*GPP_mask[np.newaxis,:,:]\n",
    "T_S_noISA_Reco=S_noISA_Reco[:,::-1]*GPP_mask[np.newaxis,:,:]\n",
    "T_S_noISA_NEE=S_noISA_NEE[:,::-1]*GPP_mask[np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap the axes of UrbanVPRM fluxes to match that of SMUrF data and apply Toronto mask\n",
    "T_VPRM_NEE=np.swapaxes(np.swapaxes(T_VPRM_NEE,0,2),1,2)*GPP_mask[np.newaxis,:,:]\n",
    "T_VPRM_Reco=np.swapaxes(np.swapaxes(T_VPRM_Reco,0,2),1,2)*GPP_mask[np.newaxis,:,:]\n",
    "T_VPRM_GPP=np.swapaxes(np.swapaxes(T_VPRM_GPP,0,2),1,2)*GPP_mask[np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Apply a 100 times bootstrapped Hubber fit for each hour of the year to find correlation between SMUrF and UrbanVPRM NEE\n",
    "\n",
    "Huber_NEE_w_int_R2=np.zeros(8760)*np.nan\n",
    "Huber_NEE_w_int_slp=np.zeros(8760)*np.nan\n",
    "Huber_NEE_int=np.zeros(8760)*np.nan\n",
    "Huber_NEE_w_int_slp_err=np.zeros(8760)*np.nan\n",
    "Huber_NEE_int_err=np.zeros(8760)*np.nan\n",
    "\n",
    "NEE_1_1_RMSE = np.zeros(8760)*np.nan\n",
    "NEE_1_1_MBE = np.zeros(8760)*np.nan\n",
    "\n",
    "for i in range(len(T_VPRM_NEE)):\n",
    "    S_NEE_i = T_S_NEE[i]\n",
    "    VPRM_NEE_i = T_VPRM_NEE[i]\n",
    "    \n",
    "    finitemask0=np.isfinite(S_NEE_i) & np.isfinite(VPRM_NEE_i)\n",
    "    S_NEE_clean0=S_NEE_i[finitemask0]\n",
    "    VPRM_NEE_clean0=VPRM_NEE_i[finitemask0]\n",
    "\n",
    "    NEE_1_1_RMSE[i] = np.sqrt(np.sum((VPRM_NEE_clean0-S_NEE_clean0)**2)/(len(S_NEE_clean0)))                                \n",
    "    NEE_1_1_MBE[i] = np.mean(VPRM_NEE_clean0-S_NEE_clean0)                                \n",
    "    \n",
    "    Huber_2018_slps=[]\n",
    "    Huber_2018_ints=[]\n",
    "    Huber_2018_R2=[]\n",
    "\n",
    "    #try bootstrapping\n",
    "    indx_list=list(range(0,len(S_NEE_clean0)))\n",
    "    for j in range(0,100):\n",
    "        #sub selection of points\n",
    "        S_NEE_indx=np.random.choice(indx_list,size=len(S_NEE_clean0))\n",
    "    \n",
    "        try:\n",
    "            Huber_model = linear_model.HuberRegressor(fit_intercept=True)\n",
    "            Huber_fit=Huber_model.fit((S_NEE_clean0[S_NEE_indx]).reshape(-1,1),(VPRM_NEE_clean0[S_NEE_indx]))\n",
    "            H_m=Huber_fit.coef_\n",
    "            H_c=Huber_fit.intercept_\n",
    "            y_predict = H_m * S_NEE_clean0 + H_c\n",
    "            H_R2=r2_score(VPRM_NEE_clean0, y_predict)\n",
    "            Huber_2018_slps.append(H_m)\n",
    "            Huber_2018_ints.append(H_c)\n",
    "            Huber_2018_R2.append(H_R2)\n",
    "        except ValueError: #if Huber fit can't find a solution for the subset, skip it\n",
    "            pass\n",
    "    \n",
    "    Huber_NEE_w_int_slp[i] = np.nanmean(Huber_2018_slps)\n",
    "    Huber_NEE_w_int_slp_err[i] = np.nanstd(Huber_2018_slps)\n",
    "    Huber_NEE_int[i] = np.nanmean(Huber_2018_ints)\n",
    "    Huber_NEE_int_err[i] = np.nanstd(Huber_2018_ints)\n",
    "    Huber_NEE_w_int_R2[i]=r2_score(VPRM_NEE_clean0, Huber_NEE_w_int_slp[i] * S_NEE_clean0 + Huber_NEE_int[i])\n",
    "    if i%100==0: #Can comment out progress bar\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *** Uncoment to show that 100 bootstraps is sufficient (values converge)\n",
    "#Huber_mv_avg_slp=[]\n",
    "#Huber_mv_std_slp=[]\n",
    "#Huber_mv_avg_int=[]\n",
    "#Huber_mv_std_int=[]\n",
    "#for m in range(1,len(Huber_2018_slps)+1):\n",
    "#    Huber_mv_avg_slp.append(np.nanmean(Huber_2018_slps[0:m]))\n",
    "#    Huber_mv_std_slp.append(np.nanstd(Huber_2018_slps[0:m]))\n",
    "#    Huber_mv_avg_int.append(np.nanmean(Huber_2018_ints[0:m]))\n",
    "#    Huber_mv_std_int.append(np.nanstd(Huber_2018_ints[0:m]))\n",
    "\n",
    "#plt.scatter(np.arange(0,100),Huber_2018_slps)\n",
    "#plt.scatter(np.arange(0,100),Huber_mv_avg_slp)\n",
    "#plt.axhline(Huber_NEE_w_int_slp[i],linestyle='--',c='k') #average slope\n",
    "#plt.show()\n",
    "\n",
    "#plt.scatter(np.arange(0,100),Huber_mv_std_slp)\n",
    "#plt.axhline(Huber_NEE_w_int_slp_err[i],linestyle='--',c='k') #average error in slope\n",
    "#plt.show()\n",
    "\n",
    "#plt.scatter(np.arange(0,100),Huber_2018_ints)\n",
    "#plt.scatter(np.arange(0,100),Huber_mv_avg_int)\n",
    "#plt.axhline(Huber_NEE_int[i],linestyle='--',c='k') #average intercept\n",
    "#plt.show()\n",
    "\n",
    "#plt.scatter(np.arange(0,100),Huber_mv_std_int)\n",
    "#plt.axhline(Huber_NEE_int_err[i],linestyle='--',c='k') #average error in intercept\n",
    "#plt.show()\n",
    "\n",
    "##end of uncomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Huber_NEE_noISA_w_int_R2=np.zeros(8760)*np.nan\n",
    "Huber_NEE_noISA_w_int_slp=np.zeros(8760)*np.nan\n",
    "Huber_NEE_noISA_int=np.zeros(8760)*np.nan\n",
    "Huber_NEE_noISA_w_int_slp_err=np.zeros(8760)*np.nan\n",
    "Huber_NEE_noISA_int_err=np.zeros(8760)*np.nan\n",
    "\n",
    "NEE_noISA_1_1_RMSE = np.zeros(8760)*np.nan\n",
    "NEE_noISA_1_1_MBE = np.zeros(8760)*np.nan\n",
    "\n",
    "for i in range(len(T_VPRM_NEE)):\n",
    "    S_noISA_NEE_i = T_S_noISA_NEE[i]\n",
    "    VPRM_NEE_i = T_VPRM_NEE[i]\n",
    "    \n",
    "    finitemask0=np.isfinite(S_noISA_NEE_i) & np.isfinite(VPRM_NEE_i)\n",
    "    S_noISA_NEE_clean0=S_noISA_NEE_i[finitemask0]\n",
    "    VPRM_NEE_clean0=VPRM_NEE_i[finitemask0]\n",
    "\n",
    "    NEE_noISA_1_1_RMSE[i] = np.sqrt(np.sum((VPRM_NEE_clean0-S_noISA_NEE_clean0)**2)/(len(S_noISA_NEE_clean0)))                                \n",
    "    NEE_noISA_1_1_MBE[i] = np.mean(VPRM_NEE_clean0-S_noISA_NEE_clean0)                                \n",
    "    \n",
    "    Huber_noISA_2018_slps=[]\n",
    "    Huber_noISA_2018_ints=[]\n",
    "    Huber_noISA_2018_R2=[]\n",
    "\n",
    "    #try bootstrapping\n",
    "    indx_list=list(range(0,len(S_noISA_NEE_clean0)))\n",
    "    for j in range(0,100):\n",
    "        #sub selection of points\n",
    "        S_noISA_NEE_indx=np.random.choice(indx_list,size=len(S_noISA_NEE_clean0))\n",
    "    \n",
    "        try:\n",
    "            Huber_model = linear_model.HuberRegressor(fit_intercept=True)\n",
    "            Huber_fit=Huber_model.fit((S_noISA_NEE_clean0[S_noISA_NEE_indx]).reshape(-1,1),(VPRM_NEE_clean0[S_noISA_NEE_indx]))\n",
    "            H_m=Huber_fit.coef_\n",
    "            H_c=Huber_fit.intercept_\n",
    "            y_predict = H_m * S_noISA_NEE_clean0 + H_c\n",
    "            H_R2=r2_score(VPRM_NEE_clean0, y_predict)\n",
    "            Huber_noISA_2018_slps.append(H_m)\n",
    "            Huber_noISA_2018_ints.append(H_c)\n",
    "            Huber_noISA_2018_R2.append(H_R2)\n",
    "        except ValueError: #if Huber fit can't find a solution for the subset, skip it\n",
    "            pass\n",
    "    \n",
    "    Huber_NEE_noISA_w_int_slp[i] = np.nanmean(Huber_noISA_2018_slps)\n",
    "    Huber_NEE_noISA_w_int_slp_err[i] = np.nanstd(Huber_noISA_2018_slps)\n",
    "    Huber_NEE_noISA_int[i] = np.nanmean(Huber_noISA_2018_ints)\n",
    "    Huber_NEE_noISA_int_err[i] = np.nanstd(Huber_noISA_2018_ints)\n",
    "    Huber_NEE_noISA_w_int_R2[i]=r2_score(VPRM_NEE_clean0, Huber_NEE_noISA_w_int_slp[i] * S_noISA_NEE_clean0 + Huber_NEE_noISA_int[i])\n",
    "    if i%100==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Huber_GPP_w_int_R2=np.zeros(8760)*np.nan\n",
    "Huber_GPP_w_int_slp=np.zeros(8760)*np.nan\n",
    "Huber_GPP_int=np.zeros(8760)*np.nan\n",
    "Huber_GPP_w_int_slp_err=np.zeros(8760)*np.nan\n",
    "Huber_GPP_int_err=np.zeros(8760)*np.nan\n",
    "\n",
    "GPP_1_1_RMSE = np.zeros(8760)*np.nan\n",
    "GPP_1_1_MBE = np.zeros(8760)*np.nan\n",
    "\n",
    "for i in range(len(T_VPRM_GPP)):\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        S_GPP_i = T_S_GPP[i]\n",
    "        VPRM_GPP_i = T_VPRM_GPP[i]\n",
    "\n",
    "        finitemask0=np.isfinite(S_GPP_i) & np.isfinite(VPRM_GPP_i)\n",
    "        S_GPP_clean0=S_GPP_i[finitemask0]\n",
    "        VPRM_GPP_clean0=VPRM_GPP_i[finitemask0]\n",
    "\n",
    "        try:\n",
    "            GPP_1_1_RMSE[i] = np.sqrt(np.sum((VPRM_GPP_clean0-S_GPP_clean0)**2)/(len(S_GPP_clean0)))                                \n",
    "            GPP_1_1_MBE[i] = np.mean(VPRM_GPP_clean0-S_GPP_clean0)                                \n",
    "        except RuntimeError:\n",
    "            pass\n",
    "\n",
    "        Huber_2018_slps=[]\n",
    "        Huber_2018_ints=[]\n",
    "        Huber_2018_R2=[]\n",
    "\n",
    "        #try bootstrapping\n",
    "        indx_list=list(range(0,len(S_GPP_clean0)))\n",
    "        for j in range(0,100):\n",
    "            #sub selection of points\n",
    "            S_GPP_indx=np.random.choice(indx_list,size=len(S_GPP_clean0))\n",
    "\n",
    "            try:\n",
    "                Huber_model = linear_model.HuberRegressor(fit_intercept=True)\n",
    "                Huber_fit=Huber_model.fit((S_GPP_clean0[S_GPP_indx]).reshape(-1,1),(VPRM_GPP_clean0[S_GPP_indx]))\n",
    "                H_m=Huber_fit.coef_\n",
    "                H_c=Huber_fit.intercept_\n",
    "                y_predict = H_m * S_GPP_clean0 + H_c\n",
    "                H_R2=r2_score(VPRM_GPP_clean0, y_predict)\n",
    "                Huber_2018_slps.append(H_m)\n",
    "                Huber_2018_ints.append(H_c)\n",
    "                Huber_2018_R2.append(H_R2)\n",
    "            except ValueError: #if Huber fit can't find a solution for the subset, skip it\n",
    "                pass\n",
    "\n",
    "        Huber_GPP_w_int_slp[i] = np.nanmean(Huber_2018_slps)\n",
    "        Huber_GPP_w_int_slp_err[i] = np.nanstd(Huber_2018_slps)\n",
    "        Huber_GPP_int[i] = np.nanmean(Huber_2018_ints)\n",
    "        Huber_GPP_int_err[i] = np.nanstd(Huber_2018_ints)\n",
    "        Huber_GPP_w_int_R2[i]=r2_score(VPRM_GPP_clean0, Huber_GPP_w_int_slp[i] * S_GPP_clean0 + Huber_GPP_int[i])\n",
    "        if i%100==0:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When GPP is 0 for both UrbanVPRM and SMUrF R2 becomes 1 and slope gets all wonky, remove these values & replace with nan\n",
    "\n",
    "Huber_GPP_w_int_slp[Huber_GPP_w_int_R2==1]=np.nan\n",
    "Huber_GPP_w_int_slp_err[Huber_GPP_w_int_R2==1]=np.nan\n",
    "Huber_GPP_int[Huber_GPP_w_int_R2==1]=np.nan\n",
    "Huber_GPP_int_err[Huber_GPP_w_int_R2==1]=np.nan\n",
    "Huber_GPP_w_int_R2[Huber_GPP_w_int_R2==1]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Huber_Reco_w_int_R2=np.zeros(8760)*np.nan\n",
    "Huber_Reco_w_int_slp=np.zeros(8760)*np.nan\n",
    "Huber_Reco_int=np.zeros(8760)*np.nan\n",
    "Huber_Reco_w_int_slp_err=np.zeros(8760)*np.nan\n",
    "Huber_Reco_int_err=np.zeros(8760)*np.nan\n",
    "\n",
    "Reco_1_1_RMSE = np.zeros(8760)*np.nan\n",
    "Reco_1_1_MBE = np.zeros(8760)*np.nan\n",
    "\n",
    "for i in range(len(T_VPRM_Reco)):\n",
    "    S_Reco_i = T_S_Reco[i]\n",
    "    VPRM_Reco_i = T_VPRM_Reco[i]\n",
    "    \n",
    "    finitemask0=np.isfinite(S_Reco_i) & np.isfinite(VPRM_Reco_i)\n",
    "    S_Reco_clean0=S_Reco_i[finitemask0]\n",
    "    VPRM_Reco_clean0=VPRM_Reco_i[finitemask0]\n",
    "\n",
    "    Reco_1_1_RMSE[i] = np.sqrt(np.sum((VPRM_Reco_clean0-S_Reco_clean0)**2)/(len(S_Reco_clean0)))                                \n",
    "    Reco_1_1_MBE[i] = np.mean(VPRM_Reco_clean0-S_Reco_clean0)                                \n",
    "    \n",
    "    Huber_2018_slps=[]\n",
    "    Huber_2018_ints=[]\n",
    "    Huber_2018_R2=[]\n",
    "\n",
    "    #try bootstrapping\n",
    "    indx_list=list(range(0,len(S_Reco_clean0)))\n",
    "    for j in range(0,100):\n",
    "        #sub selection of points\n",
    "        S_Reco_indx=np.random.choice(indx_list,size=len(S_Reco_clean0))\n",
    "    \n",
    "        try:\n",
    "            Huber_model = linear_model.HuberRegressor(fit_intercept=True)\n",
    "            Huber_fit=Huber_model.fit((S_Reco_clean0[S_Reco_indx]).reshape(-1,1),(VPRM_Reco_clean0[S_Reco_indx]))\n",
    "            H_m=Huber_fit.coef_\n",
    "            H_c=Huber_fit.intercept_\n",
    "            y_predict = H_m * S_Reco_clean0 + H_c\n",
    "            H_R2=r2_score(VPRM_Reco_clean0, y_predict)\n",
    "            Huber_2018_slps.append(H_m)\n",
    "            Huber_2018_ints.append(H_c)\n",
    "            Huber_2018_R2.append(H_R2)\n",
    "        except ValueError: #if Huber fit can't find a solution for the subset, skip it\n",
    "            pass\n",
    "    \n",
    "    Huber_Reco_w_int_slp[i] = np.nanmean(Huber_2018_slps)\n",
    "    Huber_Reco_w_int_slp_err[i] = np.nanstd(Huber_2018_slps)\n",
    "    Huber_Reco_int[i] = np.nanmean(Huber_2018_ints)\n",
    "    Huber_Reco_int_err[i] = np.nanstd(Huber_2018_ints)\n",
    "    Huber_Reco_w_int_R2[i]=r2_score(VPRM_Reco_clean0, Huber_Reco_w_int_slp[i] * S_Reco_clean0 + Huber_Reco_int[i])\n",
    "    if i%100==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Huber_Reco_noISA_w_int_R2=np.zeros(8760)*np.nan\n",
    "Huber_Reco_noISA_w_int_slp=np.zeros(8760)*np.nan\n",
    "Huber_Reco_noISA_int=np.zeros(8760)*np.nan\n",
    "Huber_Reco_noISA_w_int_slp_err=np.zeros(8760)*np.nan\n",
    "Huber_Reco_noISA_int_err=np.zeros(8760)*np.nan\n",
    "\n",
    "Reco_noISA_1_1_RMSE = np.zeros(8760)*np.nan\n",
    "Reco_noISA_1_1_MBE = np.zeros(8760)*np.nan\n",
    "\n",
    "for i in range(len(T_VPRM_Reco)):\n",
    "    S_noISA_Reco_i = T_S_noISA_Reco[i]\n",
    "    VPRM_Reco_i = T_VPRM_Reco[i]\n",
    "    \n",
    "    finitemask0=np.isfinite(S_noISA_Reco_i) & np.isfinite(VPRM_Reco_i)\n",
    "    S_noISA_Reco_clean0=S_noISA_Reco_i[finitemask0]\n",
    "    VPRM_Reco_clean0=VPRM_Reco_i[finitemask0]\n",
    "\n",
    "    Reco_noISA_1_1_RMSE[i] = np.sqrt(np.sum((VPRM_Reco_clean0-S_noISA_Reco_clean0)**2)/(len(S_noISA_Reco_clean0)))                                \n",
    "    Reco_noISA_1_1_MBE[i] = np.mean(VPRM_Reco_clean0-S_noISA_Reco_clean0)                                \n",
    "    \n",
    "    Huber_noISA_2018_slps=[]\n",
    "    Huber_noISA_2018_ints=[]\n",
    "    Huber_noISA_2018_R2=[]\n",
    "\n",
    "    #try bootstrapping\n",
    "    indx_list=list(range(0,len(S_noISA_Reco_clean0)))\n",
    "    for j in range(0,100):\n",
    "        #sub selection of points\n",
    "        S_noISA_Reco_indx=np.random.choice(indx_list,size=len(S_noISA_Reco_clean0))\n",
    "    \n",
    "        try:\n",
    "            Huber_model = linear_model.HuberRegressor(fit_intercept=True)\n",
    "            Huber_fit=Huber_model.fit((S_noISA_Reco_clean0[S_noISA_Reco_indx]).reshape(-1,1),(VPRM_Reco_clean0[S_noISA_Reco_indx]))\n",
    "            H_m=Huber_fit.coef_\n",
    "            H_c=Huber_fit.intercept_\n",
    "            y_predict = H_m * S_noISA_Reco_clean0 + H_c\n",
    "            H_R2=r2_score(VPRM_Reco_clean0, y_predict)\n",
    "            Huber_noISA_2018_slps.append(H_m)\n",
    "            Huber_noISA_2018_ints.append(H_c)\n",
    "            Huber_noISA_2018_R2.append(H_R2)\n",
    "        except ValueError: #if Huber fit can't find a solution for the subset, skip it\n",
    "            pass\n",
    "    \n",
    "    Huber_Reco_noISA_w_int_slp[i] = np.nanmean(Huber_noISA_2018_slps)\n",
    "    Huber_Reco_noISA_w_int_slp_err[i] = np.nanstd(Huber_noISA_2018_slps)\n",
    "    Huber_Reco_noISA_int[i] = np.nanmean(Huber_noISA_2018_ints)\n",
    "    Huber_Reco_noISA_int_err[i] = np.nanstd(Huber_noISA_2018_ints)\n",
    "    Huber_Reco_noISA_w_int_R2[i]=r2_score(VPRM_Reco_clean0, Huber_Reco_noISA_w_int_slp[i] * S_noISA_Reco_clean0 + Huber_Reco_noISA_int[i])\n",
    "    if i%100==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HoY = np.arange(1,8761)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "plt.rc('font',size=27) #21\n",
    "\n",
    "fig, ax = plt.subplots(3,3,sharex=True,sharey='row',figsize=(24,12))\n",
    "ax[0,0].set_xlim(1,366)\n",
    "ax[0,0].set_ylim(-0.01,0.99)\n",
    "\n",
    "fig0=ax[0,0].scatter(HoY/24+1,Huber_NEE_noISA_w_int_R2,s=5)\n",
    "ax[0,0].scatter(HoY/24+1,Huber_NEE_w_int_R2,s=5)\n",
    "ax[0,1].scatter(HoY/24+1,Huber_Reco_noISA_w_int_R2,s=5)\n",
    "ax[0,1].scatter(HoY/24+1,Huber_Reco_w_int_R2,s=5)\n",
    "ax[0,2].scatter(HoY/24+1,Huber_GPP_w_int_R2,s=5)\n",
    "\n",
    "ax[0,0].set_title('NEE Correlation Statistics')\n",
    "ax[0,1].set_title('R$_{eco}$ Correlation Statistics')\n",
    "ax[0,2].set_title('GPP Correlation Statistics')\n",
    "\n",
    "ax[1,0].set_ylim(-0.5,12)\n",
    "fig1=ax[1,0].scatter(HoY/24+1,NEE_noISA_1_1_RMSE,s=5)\n",
    "ax[1,0].scatter(HoY/24+1,NEE_1_1_RMSE,s=5)\n",
    "ax[1,1].scatter(HoY/24+1,Reco_noISA_1_1_RMSE,s=5)\n",
    "ax[1,1].scatter(HoY/24+1,Reco_1_1_RMSE,s=5)\n",
    "ax[1,2].scatter(HoY/24+1,GPP_1_1_RMSE,s=5)\n",
    "\n",
    "ax[1,1].scatter([0],[0],c='#006BA4',label='Without ISA Adjustment')\n",
    "ax[1,1].scatter([0],[0],c='#FF800E',label='With ISA Adjustment')\n",
    "ax[1,1].legend(loc=(0.025,0.375))#fontsize=22)\n",
    "\n",
    "\n",
    "ax[2,0].set_ylim(-9,9)\n",
    "fig2=ax[2,0].scatter(HoY/24+1,NEE_noISA_1_1_MBE,s=5)\n",
    "ax[2,0].scatter(HoY/24+1,NEE_1_1_MBE,s=5)\n",
    "ax[2,1].scatter(HoY/24+1,Reco_noISA_1_1_MBE,s=5)\n",
    "ax[2,1].scatter(HoY/24+1,Reco_1_1_MBE,s=5)\n",
    "ax[2,2].scatter(HoY/24+1,GPP_1_1_MBE,s=5)\n",
    "ax[2,0].axhline(0,linestyle='--',c='k')\n",
    "ax[2,1].axhline(0,linestyle='--',c='k')\n",
    "ax[2,2].axhline(0,linestyle='--',c='k')\n",
    "\n",
    "ax[0,0].set_ylabel('R$^2$')\n",
    "ax[1,0].set_ylabel('RMSE')# ($\\mu$mol m$^{-2}$ s$^{-1}$)')\n",
    "ax[2,0].set_ylabel('MBE')# ($\\mu$mol m$^{-2}$ s$^{-1}$)')\n",
    "\n",
    "\n",
    "ax[2,0].set_xlabel('Day of Year')\n",
    "ax[2,1].set_xlabel('Day of Year')\n",
    "ax[2,2].set_xlabel('Day of Year')\n",
    "\n",
    "ax[0,0].text(4,0.875,'(a)',c='k',fontsize=26)\n",
    "ax[0,1].text(4,0.875,'(b)',c='k',fontsize=26)\n",
    "ax[0,2].text(4,0.875,'(c)',c='k',fontsize=26)\n",
    "ax[1,0].text(4,10.5,'(d)',c='k',fontsize=26)\n",
    "ax[1,1].text(4,10.5,'(e)',c='k',fontsize=26)\n",
    "ax[1,2].text(4,10.5,'(f)',c='k',fontsize=26)\n",
    "ax[2,0].text(4,6.75,'(g)',c='k',fontsize=26)\n",
    "ax[2,1].text(4,6.75,'(h)',c='k',fontsize=26)\n",
    "ax[2,2].text(4,6.75,'(i)',c='k',fontsize=26)\n",
    "\n",
    "fig.subplots_adjust(hspace=0,wspace=0)\n",
    "# *** Uncomment to save figure as pdf & png. CHANGE FILENAMES ***\n",
    "#plt.savefig('Fixed_SMUrF_shore_corr_UrbanVPRM_before_after_ISA_correction_Huber_correlation_stats_all_fluxes_cb_friendly_larger_font_labelled.pdf',bbox_inches='tight')\n",
    "#plt.savefig('Fixed_SMUrF_shore_corr_UrbanVPRM_before_after_ISA_correction_Huber_correlation_stats_all_fluxes_cb_friendly_larger_font_labelled.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
