{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code compares & fits biogenic fluxes from the original and updated SMUrF model to 3 eddy covariance fluxtowers\n",
    "# for each season of 2018 & 2019.\n",
    "\n",
    "#  This creates Figs. 3 d-f of Madsen-Colford et al. 2025\n",
    "\n",
    "# *** denotes areas that the user should change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #numerical python\n",
    "import matplotlib.pyplot as plt #for plotting\n",
    "from matplotlib.cm import get_cmap #import colour maps for contour plots\n",
    "import netCDF4\n",
    "from netCDF4 import Dataset, date2num #for reading netCDF data files and their date (not sure if I need the later)\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy import optimize as opt \n",
    "from scipy import odr\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "#from datetime import datetime as dt\n",
    "import matplotlib.patheffects as pe\n",
    "from sklearn import linear_model #for robust fitting\n",
    "from sklearn.metrics import r2_score, mean_squared_error #for analyzing robust fits\n",
    "import matplotlib.colors as clrs #for log color scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import first CSIF data file to get the start of the year & convert to days since 1970\n",
    "\n",
    "# *** CHANGE PATH & FILENAME ***\n",
    "g=Dataset('E:/Research/SMUrF/output2018_CSIF_V061/easternCONUS/daily_mean_Reco_neuralnet/era5/2018/daily_mean_Reco_uncert_easternCONUS_20180101.nc')\n",
    "start_of_year=g.variables['time'][0]/3600/24-1 #convert seconds since 1970 to days (minus one)\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** CHANGE PATH ***\n",
    "C_path = 'E:/Research/SMUrF/output2018_CSIF_V061/easternCONUS/hourly_flux_era5/'\n",
    "# *** CHANGE FILENAME ***\n",
    "C_fn = 'hrly_mean_GPP_Reco_NEE_easternCONUS_2018'\n",
    "\n",
    "C_time=[]\n",
    "C_Reco=[]\n",
    "C_NEE=[]\n",
    "C_GPP=[]\n",
    "C_lats=[]\n",
    "C_lons=[]\n",
    "for j in range(1,13):\n",
    "    try:\n",
    "        #Import original SMUrF data (using CSIF)\n",
    "        if j<10:\n",
    "            f=Dataset(C_path+C_fn+'0'+str(j)+'.nc')\n",
    "        else:\n",
    "            # *** CHANGE PATH ***\n",
    "            f=Dataset(C_path+C_fn+str(j)+'.nc')\n",
    "        if len(C_time)==0:\n",
    "            # If it is the first file create arrays for each variable and save lat/lon\n",
    "            C_lats=f.variables['lat'][:]\n",
    "            C_lons=f.variables['lon'][:]\n",
    "            C_Reco=f.variables['Reco_mean'][:]\n",
    "            C_GPP=f.variables['GPP_mean'][:]\n",
    "            C_NEE=f.variables['NEE_mean'][:]\n",
    "            C_time=f.variables['time'][:]/24/3600-start_of_year-5/24 #convert seconds since 1970 to days and subtract start of year and adjust to local time\n",
    "        else:\n",
    "            # Otherwise append the data to the arrays\n",
    "            C_Reco=np.concatenate((C_Reco,f.variables['Reco_mean'][:]),axis=0)\n",
    "            C_GPP=np.concatenate((C_GPP,f.variables['GPP_mean'][:]),axis=0)\n",
    "            C_NEE=np.concatenate((C_NEE,f.variables['NEE_mean'][:]),axis=0)\n",
    "            C_time=np.concatenate((C_time,(f.variables['time'][:]/24/3600-start_of_year-5/24)),axis=0)\n",
    "        f.close()\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace fill values with NaN\n",
    "C_Reco[C_Reco==-999]=np.nan\n",
    "C_NEE[C_NEE==-999]=np.nan\n",
    "C_GPP[C_GPP==-999]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the pixel over Borden forest\n",
    "C_GPP_array=np.zeros(8765)*np.nan\n",
    "C_NEE_array=np.zeros(8765)*np.nan\n",
    "C_Reco_array=np.zeros(8765)*np.nan\n",
    "C_time_array=np.zeros(8765)*np.nan\n",
    "for i in range(len(C_GPP[:,0,0])):\n",
    "    C_time_array[i]=C_time[i]\n",
    "    C_GPP_array[i]=C_GPP[i,36,15]\n",
    "    C_NEE_array[i]=C_NEE[i,36,15]\n",
    "    C_Reco_array[i]=C_Reco[i,36,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Borden Fluxtower data\n",
    "\n",
    "# *** CHANGE PATH ***\n",
    "Borden_Fluxes=pd.read_csv('/Users/kitty/Documents/Research/SIF/Flux_Tower/2018_NEP_GPP_Borden.csv', index_col=0)\n",
    "\n",
    "#Extract Borden flux data into arrays:\n",
    "Borden_dates_fluxes=np.zeros([17520])*np.nan\n",
    "Borden_NEEgf_fluxes=np.zeros([17520])*np.nan\n",
    "Borden_NEE_fluxes=np.zeros([17520])*np.nan\n",
    "Borden_R_fluxes=np.zeros([17520])*np.nan\n",
    "Borden_Rgf_fluxes=np.zeros([17520])*np.nan\n",
    "Borden_GEP_fluxes=np.zeros([17520])*np.nan\n",
    "Borden_GEPgf_fluxes=np.zeros([17520])*np.nan\n",
    "n=0\n",
    "m=0\n",
    "date=1\n",
    "for i in range(0,17520):\n",
    "    Borden_dates_fluxes[i]=Borden_Fluxes.iat[i,0]-5/24 #adjust to local time\n",
    "    Borden_NEEgf_fluxes[i]=-Borden_Fluxes.iat[i,5] # NEE (gap filled)\n",
    "    Borden_NEE_fluxes[i]=-Borden_Fluxes.iat[i,1] # NEE (non-gap filled)\n",
    "    Borden_Rgf_fluxes[i]=Borden_Fluxes.iat[i,6]\n",
    "    Borden_R_fluxes[i]=Borden_Fluxes.iat[i,3]\n",
    "    Borden_GEP_fluxes[i]=Borden_Fluxes.iat[i,4]\n",
    "    Borden_GEPgf_fluxes[i]=Borden_Fluxes.iat[i,7]\n",
    "    \n",
    "del Borden_Fluxes #Remove remaining data to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert half-hourly flux tower data to hourly averages to match resolution of SMUrF \n",
    "# Average hour and the next half hour period\n",
    "Borden_NEE=np.zeros(np.shape(C_GPP_array))*np.nan\n",
    "Borden_GEPgf=np.zeros(np.shape(C_GPP_array))*np.nan\n",
    "Borden_NEEgf=np.zeros(np.shape(C_GPP_array))*np.nan\n",
    "Borden_Rgf=np.zeros(np.shape(C_GPP_array))*np.nan\n",
    "for i in range(np.int(len(Borden_dates_fluxes)/2)):\n",
    "    Borden_NEE[i]=np.nanmean([Borden_NEE_fluxes[i*2],Borden_NEE_fluxes[i*2+1]])\n",
    "    Borden_GEPgf[i]=np.nanmean([Borden_GEPgf_fluxes[i*2],Borden_GEPgf_fluxes[i*2+1]])\n",
    "    Borden_NEEgf[i]=np.nanmean([Borden_NEEgf_fluxes[i*2],Borden_NEEgf_fluxes[i*2+1]])\n",
    "    Borden_Rgf[i]=np.nanmean([Borden_Rgf_fluxes[i*2],Borden_Rgf_fluxes[i*2+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load in SMUrF data and crop to flux tower locations for 2018:\n",
    "\n",
    "S_time=[]\n",
    "S_lats=[]\n",
    "S_lons=[]\n",
    "\n",
    "S_Reco_Borden=[]\n",
    "S_Reco_std_Borden=[]\n",
    "S_NEE_Borden=[]\n",
    "S_NEE_std_Borden=[]\n",
    "S_GPP_Borden=[]\n",
    "S_GPP_std_Borden=[]\n",
    "\n",
    "S_Reco_TP39=[]\n",
    "S_Reco_std_TP39=[]\n",
    "S_NEE_TP39=[]\n",
    "S_NEE_std_TP39=[]\n",
    "S_GPP_TP39=[]\n",
    "S_GPP_std_TP39=[]\n",
    "\n",
    "S_Reco_TPD=[]\n",
    "S_Reco_std_TPD=[]\n",
    "S_NEE_TPD=[]\n",
    "S_NEE_std_TPD=[]\n",
    "S_GPP_TPD=[]\n",
    "S_GPP_std_TPD=[]\n",
    "\n",
    "# *** CHANGE PATH ***\n",
    "S_path = 'C:/Users/kitty/Documents/Research/SIF/SMUrF/output2018_500m_CSIF_to_TROPOMI_CSIF_ALL_converted_slps_V3_temp_impervious_R_shore_corr_V061_8day/easternCONUS/hourly_flux_GMIS_combined_ISA_a_w_sd_era5/'\n",
    "# *** CHANGE FILENAME ***\n",
    "S_fn = 'hrly_mean_GPP_Reco_NEE_easternCONUS_2018' # filename (without the month)\n",
    "\n",
    "for j in range(1,13):\n",
    "    try:\n",
    "        #load in the data\n",
    "        if j<10:\n",
    "            f=Dataset(S_path+S_fn+'0'+str(j)+'.nc')\n",
    "        else:\n",
    "            f=Dataset(S_path+S_fn+str(j)+'.nc')\n",
    "        \n",
    "        S_Reco=f.variables['Reco_mean'][:]\n",
    "        S_GPP=f.variables['GPP_mean'][:]\n",
    "        S_NEE=f.variables['NEE_mean'][:]\n",
    "        \n",
    "        if len(S_time)==0:\n",
    "            # If it is the first file start an array for each variable and save lat/lon & fluxes\n",
    "            S_lats=f.variables['lat'][:]\n",
    "            S_lons=f.variables['lon'][:]\n",
    "            S_time=f.variables['time'][:]/24/3600-start_of_year-5/24 #convert seconds since 1970 to days and subtract start of year and adjust to local time\n",
    "                        \n",
    "            S_GPP_Borden = np.nanmean([S_GPP[:,458,230],S_GPP[:,458,231],S_GPP[:,458,232],S_GPP[:,459,230],S_GPP[:,459,231],S_GPP[:,459,232],S_GPP[:,460,232]],axis=0)\n",
    "            S_GPP_std_Borden = np.nanstd([S_GPP[:,458,230],S_GPP[:,458,231],S_GPP[:,458,232],S_GPP[:,459,230],S_GPP[:,459,231],S_GPP[:,459,232],S_GPP[:,460,232]],axis=0)\n",
    "            S_Reco_Borden = np.nanmean([S_Reco[:,458,230],S_Reco[:,458,231],S_Reco[:,458,232],S_Reco[:,459,230],S_Reco[:,459,231],S_Reco[:,459,232],S_Reco[:,460,232]],axis=0)\n",
    "            S_Reco_std_Borden = np.nanstd([S_Reco[:,458,230],S_Reco[:,458,231],S_Reco[:,458,232],S_Reco[:,459,230],S_Reco[:,459,231],S_Reco[:,459,232],S_Reco[:,460,232]],axis=0)\n",
    "            S_NEE_Borden = np.nanmean([S_NEE[:,458,230],S_NEE[:,458,231],S_NEE[:,458,232],S_NEE[:,459,230],S_NEE[:,459,231],S_NEE[:,459,232],S_NEE[:,460,232]],axis=0)\n",
    "            S_NEE_std_Borden = np.nanstd([S_NEE[:,458,230],S_NEE[:,458,231],S_NEE[:,458,232],S_NEE[:,459,230],S_NEE[:,459,231],S_NEE[:,459,232],S_NEE[:,460,232]],axis=0)\n",
    "\n",
    "            S_GPP_TP39 = np.nanmean(S_GPP[:,73:75,129:131],axis=(1,2))\n",
    "            S_GPP_std_TP39 = np.nanstd(S_GPP[:,73:75,129:131],axis=(1,2))\n",
    "            S_Reco_TP39 = np.nanmean(S_Reco[:,73:75,129:131],axis=(1,2))\n",
    "            S_Reco_std_TP39 = np.nanstd(S_Reco[:,73:75,129:131],axis=(1,2))\n",
    "            S_NEE_TP39 = np.nanmean(S_NEE[:,73:75,129:131],axis=(1,2))\n",
    "            S_NEE_std_TP39 = np.nanstd(S_NEE[:,73:75,129:131],axis=(1,2))\n",
    "            \n",
    "            S_GPP_TPD = np.nanmean(S_GPP[:,55:58,80:83],axis=(1,2))\n",
    "            S_GPP_std_TPD = np.nanstd(S_GPP[:,55:58,80:83],axis=(1,2))\n",
    "            S_Reco_TPD = np.nanmean(S_Reco[:,55:58,80:83],axis=(1,2))\n",
    "            S_Reco_std_TPD = np.nanstd(S_Reco[:,55:58,80:83],axis=(1,2))\n",
    "            S_NEE_TPD = np.nanmean(S_NEE[:,55:58,80:83],axis=(1,2))\n",
    "            S_NEE_std_TPD = np.nanstd(S_NEE[:,55:58,80:83],axis=(1,2))\n",
    "            \n",
    "        else:\n",
    "            #Otherwise append fluxes to the array\n",
    "            S_GPP_Borden = np.concatenate((S_GPP_Borden,np.nanmean([S_GPP[:,458,230],S_GPP[:,458,231],S_GPP[:,458,232],S_GPP[:,459,230],S_GPP[:,459,231],S_GPP[:,459,232],S_GPP[:,460,232]],axis=0)),axis=0)\n",
    "            S_GPP_std_Borden = np.concatenate((S_GPP_std_Borden,np.nanstd([S_GPP[:,458,230],S_GPP[:,458,231],S_GPP[:,458,232],S_GPP[:,459,230],S_GPP[:,459,231],S_GPP[:,459,232],S_GPP[:,460,232]],axis=0)),axis=0)\n",
    "            S_Reco_Borden = np.concatenate((S_Reco_Borden,np.nanmean([S_Reco[:,458,230],S_Reco[:,458,231],S_Reco[:,458,232],S_Reco[:,459,230],S_Reco[:,459,231],S_Reco[:,459,232],S_Reco[:,460,232]],axis=0)),axis=0)\n",
    "            S_Reco_std_Borden = np.concatenate((S_Reco_std_Borden,np.nanstd([S_Reco[:,458,230],S_Reco[:,458,231],S_Reco[:,458,232],S_Reco[:,459,230],S_Reco[:,459,231],S_Reco[:,459,232],S_Reco[:,460,232]],axis=0)),axis=0)\n",
    "            S_NEE_Borden = np.concatenate((S_NEE_Borden,np.nanmean([S_NEE[:,458,230],S_NEE[:,458,231],S_NEE[:,458,232],S_NEE[:,459,230],S_NEE[:,459,231],S_NEE[:,459,232],S_NEE[:,460,232]],axis=0)),axis=0)\n",
    "            S_NEE_std_Borden = np.concatenate((S_NEE_std_Borden,np.nanstd([S_NEE[:,458,230],S_NEE[:,458,231],S_NEE[:,458,232],S_NEE[:,459,230],S_NEE[:,459,231],S_NEE[:,459,232],S_NEE[:,460,232]],axis=0)),axis=0)\n",
    "\n",
    "            S_GPP_TP39 = np.concatenate((S_GPP_TP39,np.nanmean(S_GPP[:,73:75,129:131],axis=(1,2))),axis=0)\n",
    "            S_GPP_std_TP39 = np.concatenate((S_GPP_std_TP39,np.nanstd(S_GPP[:,73:75,129:131],axis=(1,2))),axis=0)\n",
    "            S_Reco_TP39 = np.concatenate((S_Reco_TP39,np.nanmean(S_Reco[:,73:75,129:131],axis=(1,2))),axis=0)\n",
    "            S_Reco_std_TP39 = np.concatenate((S_Reco_std_TP39,np.nanstd(S_Reco[:,73:75,129:131],axis=(1,2))),axis=0)\n",
    "            S_NEE_TP39 = np.concatenate((S_NEE_TP39,np.nanmean(S_NEE[:,73:75,129:131],axis=(1,2))),axis=0)\n",
    "            S_NEE_std_TP39 = np.concatenate((S_NEE_std_TP39,np.nanstd(S_NEE[:,73:75,129:131],axis=(1,2))),axis=0)\n",
    "\n",
    "            S_GPP_TPD = np.concatenate((S_GPP_TPD,np.nanmean(S_GPP[:,55:58,80:83],axis=(1,2))),axis=0)\n",
    "            S_GPP_std_TPD = np.concatenate((S_GPP_std_TPD,np.nanstd(S_GPP[:,55:58,80:83],axis=(1,2))),axis=0)\n",
    "            S_Reco_TPD = np.concatenate((S_Reco_TPD,np.nanmean(S_Reco[:,55:58,80:83],axis=(1,2))),axis=0)\n",
    "            S_Reco_std_TPD = np.concatenate((S_Reco_std_TPD,np.nanstd(S_Reco[:,55:58,80:83],axis=(1,2))),axis=0)\n",
    "            S_NEE_TPD = np.concatenate((S_NEE_TPD,np.nanmean(S_NEE[:,55:58,80:83],axis=(1,2))),axis=0)\n",
    "            S_NEE_std_TPD = np.concatenate((S_NEE_std_TPD,np.nanstd(S_NEE[:,55:58,80:83],axis=(1,2))),axis=0)\n",
    "\n",
    "            S_time=np.concatenate((S_time,(f.variables['time'][:]/24/3600-start_of_year-5/24)),axis=0)\n",
    "        f.close()\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "del(S_GPP,S_Reco,S_NEE)\n",
    "\n",
    "S_GPP_Borden = np.concatenate((S_GPP_Borden,np.ones(5)*np.nan),axis=0)\n",
    "S_GPP_std_Borden = np.concatenate((S_GPP_std_Borden,np.ones(5)*np.nan),axis=0)\n",
    "S_Reco_Borden = np.concatenate((S_Reco_Borden,np.ones(5)*np.nan),axis=0)\n",
    "S_Reco_std_Borden = np.concatenate((S_Reco_std_Borden,np.ones(5)*np.nan),axis=0)\n",
    "S_NEE_Borden = np.concatenate((S_NEE_Borden,np.ones(5)*np.nan),axis=0)\n",
    "S_NEE_std_Borden = np.concatenate((S_NEE_std_Borden,np.ones(5)*np.nan),axis=0)\n",
    "\n",
    "S_GPP_TP39 = np.concatenate((S_GPP_TP39,np.ones(5)*np.nan),axis=0)\n",
    "S_GPP_std_TP39 = np.concatenate((S_GPP_std_TP39,np.ones(5)*np.nan),axis=0)\n",
    "S_Reco_TP39 = np.concatenate((S_Reco_TP39,np.ones(5)*np.nan),axis=0)\n",
    "S_Reco_std_TP39 = np.concatenate((S_Reco_std_TP39,np.ones(5)*np.nan),axis=0)\n",
    "S_NEE_TP39 = np.concatenate((S_NEE_TP39,np.ones(5)*np.nan),axis=0)\n",
    "S_NEE_std_TP39 = np.concatenate((S_NEE_std_TP39,np.ones(5)*np.nan),axis=0)\n",
    "\n",
    "S_GPP_TPD = np.concatenate((S_GPP_TPD,np.ones(5)*np.nan),axis=0)\n",
    "S_GPP_std_TPD = np.concatenate((S_GPP_std_TPD,np.ones(5)*np.nan),axis=0)\n",
    "S_Reco_TPD = np.concatenate((S_Reco_TPD,np.ones(5)*np.nan),axis=0)\n",
    "S_Reco_std_TPD = np.concatenate((S_Reco_std_TPD,np.ones(5)*np.nan),axis=0)\n",
    "S_NEE_TPD = np.concatenate((S_NEE_TPD,np.ones(5)*np.nan),axis=0)\n",
    "S_NEE_std_TPD = np.concatenate((S_NEE_std_TPD,np.ones(5)*np.nan),axis=0)\n",
    "\n",
    "S_time=np.concatenate((S_time,np.ones(5)*np.nan),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Optionally (uncomment) save the data over fluxtowers for faster loading in the future ***\n",
    "\n",
    "#g = Dataset('E:/Research/SMUrF/output2018_500m_CSIF_to_TROPOMI_CSIF_ALL_converted_slps_V3_temp_impervious_R_shore_corr_V061_8day/SMUrF_Borden_fluxes.nc','w', format='NETCDF4')\n",
    "##asif,sweight_tot,aweight_grid=aggregate(OCOf.variables['lon'][:],OCOf.variables['lat'][:], 0.05, f.variables['lon'][:],f.variables['lat'][:],f.variables['daily_sif'][:],step)\n",
    "#g.createDimension('time',len(S_time_array))\n",
    "\n",
    "## define variables to save in the file\n",
    "#NEE = g.createVariable('NEE',np.float32,'time')\n",
    "#NEE_std = g.createVariable('NEE_std',np.float32,'time')\n",
    "#GPP = g.createVariable('GPP',np.float32,'time')\n",
    "#GPP_std = g.createVariable('GPP_std',np.float32,'time')\n",
    "#Reco = g.createVariable('Reco',np.float32,'time')\n",
    "#Reco_std = g.createVariable('Reco_std',np.float32,'time')\n",
    "#t = g.createVariable('time',np.float32,'time')\n",
    "\n",
    "#NEE[:]=S_NEE_Borden\n",
    "#NEE_std[:]=S_NEE_std_Borden\n",
    "#GPP[:]=S_GPP_Borden\n",
    "#GPP_std[:]=S_GPP_std_Borden\n",
    "#Reco[:]=S_Reco_Borden\n",
    "#Reco_std[:]=S_Reco_std_Borden\n",
    "#t[:] = S_time\n",
    "##sif_error[:,:]=np.array(f.variables['Errors'][::-1])\n",
    "\n",
    "##close the file\n",
    "#g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in SMUrF data and crop to flux tower locations for 2019:\n",
    "\n",
    "S_time=[]\n",
    "S_lats=[]\n",
    "S_lons=[]\n",
    "\n",
    "S_Reco_TP39_2019=[]\n",
    "S_Reco_std_TP39_2019=[]\n",
    "S_NEE_TP39_2019=[]\n",
    "S_NEE_std_TP39_2019=[]\n",
    "S_GPP_TP39_2019=[]\n",
    "S_GPP_std_TP39_2019=[]\n",
    "\n",
    "S_Reco_TPD_2019=[]\n",
    "S_Reco_std_TPD_2019=[]\n",
    "S_NEE_TPD_2019=[]\n",
    "S_NEE_std_TPD_2019=[]\n",
    "S_GPP_TPD_2019=[]\n",
    "S_GPP_std_TPD_2019=[]\n",
    "\n",
    "# *** CHANGE PATH ***\n",
    "S_path = 'E:/Research/SMUrF/output2019_500m_CSIF_to_TROPOMI_CSIF_ALL_converted_slps_V3_temp_impervious_R_shore_corr_V061_8day/easternCONUS/hourly_flux_GMIS_Toronto_fixed_border_ISA_a_w_sd_era5/'\n",
    "# *** CHANGE FILENAME ***\n",
    "S_fn = 'hrly_mean_GPP_Reco_NEE_easternCONUS_2019' # filename (without the month)\n",
    "\n",
    "for j in range(1,13):\n",
    "    try:\n",
    "        #load in the data\n",
    "        if j<10:\n",
    "            f=Dataset(S_path+S_fn+'0'+str(j)+'.nc')\n",
    "        else:\n",
    "            f=Dataset(S_path+S_fn+str(j)+'.nc')\n",
    "        \n",
    "        S_Reco=f.variables['Reco_mean'][:]\n",
    "        S_GPP=f.variables['GPP_mean'][:]\n",
    "        S_NEE=f.variables['NEE_mean'][:]\n",
    "        \n",
    "        if len(S_time)==0:\n",
    "            # If it is the first file start an array for each variable and save lat/lon\n",
    "            S_lats=f.variables['lat'][:]\n",
    "            S_lons=f.variables['lon'][:]\n",
    "            S_time=f.variables['time'][:]/24/3600-start_of_year-5/24 #convert seconds since 1970 to days and subtract start of year and adjust to local time\n",
    "                        \n",
    "            S_GPP_TP39_2019 = np.nanmean(S_GPP[:,73:75,129:131],axis=(1,2))\n",
    "            S_GPP_std_TP39_2019 = np.nanstd(S_GPP[:,73:75,129:131],axis=(1,2))\n",
    "            S_Reco_TP39_2019 = np.nanmean(S_Reco[:,73:75,129:131],axis=(1,2))\n",
    "            S_Reco_std_TP39_2019 = np.nanstd(S_Reco[:,73:75,129:131],axis=(1,2))\n",
    "            S_NEE_TP39_2019 = np.nanmean(S_NEE[:,73:75,129:131],axis=(1,2))\n",
    "            S_NEE_std_TP39_2019 = np.nanstd(S_NEE[:,73:75,129:131],axis=(1,2))\n",
    "            \n",
    "            S_GPP_TPD_2019 = np.nanmean(S_GPP[:,55:58,80:83],axis=(1,2))\n",
    "            S_GPP_std_TPD_2019 = np.nanstd(S_GPP[:,55:58,80:83],axis=(1,2))\n",
    "            S_Reco_TPD_2019 = np.nanmean(S_Reco[:,55:58,80:83],axis=(1,2))\n",
    "            S_Reco_std_TPD_2019 = np.nanstd(S_Reco[:,55:58,80:83],axis=(1,2))\n",
    "            S_NEE_TPD_2019 = np.nanmean(S_NEE[:,55:58,80:83],axis=(1,2))\n",
    "            S_NEE_std_TPD_2019 = np.nanstd(S_NEE[:,55:58,80:83],axis=(1,2))\n",
    "            \n",
    "        else:\n",
    "            #Otherwise append to the array\n",
    "            S_GPP_TP39_2019 = np.concatenate((S_GPP_TP39_2019,np.nanmean(S_GPP[:,73:75,129:131],axis=(1,2))),axis=0)\n",
    "            S_GPP_std_TP39_2019 = np.concatenate((S_GPP_std_TP39_2019,np.nanstd(S_GPP[:,73:75,129:131],axis=(1,2))),axis=0)\n",
    "            S_Reco_TP39_2019 = np.concatenate((S_Reco_TP39_2019,np.nanmean(S_Reco[:,73:75,129:131],axis=(1,2))),axis=0)\n",
    "            S_Reco_std_TP39_2019 = np.concatenate((S_Reco_std_TP39_2019,np.nanstd(S_Reco[:,73:75,129:131],axis=(1,2))),axis=0)\n",
    "            S_NEE_TP39_2019 = np.concatenate((S_NEE_TP39_2019,np.nanmean(S_NEE[:,73:75,129:131],axis=(1,2))),axis=0)\n",
    "            S_NEE_std_TP39_2019 = np.concatenate((S_NEE_std_TP39_2019,np.nanstd(S_NEE[:,73:75,129:131],axis=(1,2))),axis=0)\n",
    "\n",
    "            S_GPP_TPD_2019 = np.concatenate((S_GPP_TPD_2019,np.nanmean(S_GPP[:,55:58,80:83],axis=(1,2))),axis=0)\n",
    "            S_GPP_std_TPD_2019 = np.concatenate((S_GPP_std_TPD_2019,np.nanstd(S_GPP[:,55:58,80:83],axis=(1,2))),axis=0)\n",
    "            S_Reco_TPD_2019 = np.concatenate((S_Reco_TPD_2019,np.nanmean(S_Reco[:,55:58,80:83],axis=(1,2))),axis=0)\n",
    "            S_Reco_std_TPD_2019 = np.concatenate((S_Reco_std_TPD_2019,np.nanstd(S_Reco[:,55:58,80:83],axis=(1,2))),axis=0)\n",
    "            S_NEE_TPD_2019 = np.concatenate((S_NEE_TPD_2019,np.nanmean(S_NEE[:,55:58,80:83],axis=(1,2))),axis=0)\n",
    "            S_NEE_std_TPD_2019 = np.concatenate((S_NEE_std_TPD_2019,np.nanstd(S_NEE[:,55:58,80:83],axis=(1,2))),axis=0)\n",
    "\n",
    "            S_time=np.concatenate((S_time,(f.variables['time'][:]/24/3600-start_of_year-5/24)),axis=0)\n",
    "        f.close()\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "del(S_GPP,S_Reco,S_NEE)\n",
    "\n",
    "S_GPP_TP39_2019 = np.concatenate((S_GPP_TP39_2019,np.ones(5)*np.nan),axis=0)\n",
    "S_GPP_std_TP39_2019 = np.concatenate((S_GPP_std_TP39_2019,np.ones(5)*np.nan),axis=0)\n",
    "S_Reco_TP39_2019 = np.concatenate((S_Reco_TP39_2019,np.ones(5)*np.nan),axis=0)\n",
    "S_Reco_std_TP39_2019 = np.concatenate((S_Reco_std_TP39_2019,np.ones(5)*np.nan),axis=0)\n",
    "S_NEE_TP39_2019 = np.concatenate((S_NEE_TP39_2019,np.ones(5)*np.nan),axis=0)\n",
    "S_NEE_std_TP39_2019 = np.concatenate((S_NEE_std_TP39_2019,np.ones(5)*np.nan),axis=0)\n",
    "\n",
    "S_GPP_TPD_2019 = np.concatenate((S_GPP_TPD_2019,np.ones(5)*np.nan),axis=0)\n",
    "S_GPP_std_TPD_2019 = np.concatenate((S_GPP_std_TPD_2019,np.ones(5)*np.nan),axis=0)\n",
    "S_Reco_TPD_2019 = np.concatenate((S_Reco_TPD_2019,np.ones(5)*np.nan),axis=0)\n",
    "S_Reco_std_TPD_2019 = np.concatenate((S_Reco_std_TPD_2019,np.ones(5)*np.nan),axis=0)\n",
    "S_NEE_TPD_2019 = np.concatenate((S_NEE_TPD_2019,np.ones(5)*np.nan),axis=0)\n",
    "S_NEE_std_TPD_2019 = np.concatenate((S_NEE_std_TPD_2019,np.ones(5)*np.nan),axis=0)\n",
    "\n",
    "S_time=np.concatenate((S_time,np.ones(5)*np.nan),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** If you have already saved the SMUrF data over flux towers uncomment this (faster): ***\n",
    "\n",
    "#g = Dataset('E:/Research/SMUrF/output2018_500m_CSIF_to_TROPOMI_CSIF_ALL_converted_slps_temp_impervious_R_V061_8day/SMUrF_Borden_fluxes.nc')\n",
    "#S_NEE_Borden=g.variables['NEE'][:]\n",
    "#S_NEE_std_Borden = g.variables['NEE_std'][:]\n",
    "#S_GPP_Borden=g.variables['GPP'][:]\n",
    "#S_GPP_std_Borden = g.variables['GPP_std'][:]\n",
    "#S_Reco_Borden=g.variables['Reco'][:]\n",
    "#S_Reco_std_Borden = g.variables['Reco_std'][:]\n",
    "#S_time=g.variables['time'][:]\n",
    "#g.close()\n",
    "\n",
    "#g = Dataset('E:/Research/SMUrF/output2018_500m_CSIF_to_TROPOMI_CSIF_ALL_converted_slps_temp_impervious_R_V061_8day/SMUrF_TP39_fluxes.nc')\n",
    "#S_NEE_TP39=g.variables['NEE'][:]\n",
    "#S_NEE_std_TP39 = g.variables['NEE_std'][:]\n",
    "#S_GPP_TP39=g.variables['GPP'][:]\n",
    "#S_GPP_std_TP39 = g.variables['GPP_std'][:]\n",
    "#S_Reco_TP39=g.variables['Reco'][:]\n",
    "#S_Reco_std_TP39 = g.variables['Reco_std'][:]\n",
    "#S_time=g.variables['time'][:]\n",
    "#g.close()\n",
    "\n",
    "#g = Dataset('E:/Research/SMUrF/output2018_500m_CSIF_to_TROPOMI_CSIF_ALL_converted_slps_temp_impervious_R_V061_8day/SMUrF_TPD_fluxes_2018.nc')\n",
    "#S_NEE_TPD=g.variables['NEE'][:]\n",
    "#S_NEE_std_TPD = g.variables['NEE_std'][:]\n",
    "#S_GPP_TPD=g.variables['GPP'][:]\n",
    "#S_GPP_std_TPD = g.variables['GPP_std'][:]\n",
    "#S_Reco_TPD=g.variables['Reco'][:]\n",
    "#S_Reco_std_TPD = g.variables['Reco_std'][:]\n",
    "#S_time=g.variables['time'][:]\n",
    "#g.close()\n",
    "\n",
    "#g = Dataset('E:/Research/SMUrF/output2019_500m_CSIF_to_TROPOMI_CSIF_ALL_converted_slps_temp_impervious_R_V061_8day/SMUrF_Borden_fluxes.nc')\n",
    "#S_NEE_Borden_2019=g.variables['NEE'][:]\n",
    "#S_NEE_2019_std_Borden = g.variables['NEE_std'][:]\n",
    "#S_GPP_Borden_2019=g.variables['GPP'][:]\n",
    "#S_GPP_2019_std_Borden = g.variables['GPP_std'][:]\n",
    "#S_Reco_Borden_2019=g.variables['Reco'][:]\n",
    "#S_Reco_2019_std_Borden = g.variables['Reco_std'][:]\n",
    "#S_time=g.variables['time'][:]\n",
    "#g.close()\n",
    "\n",
    "#g = Dataset('E:/Research/SMUrF/output2019_500m_CSIF_to_TROPOMI_CSIF_ALL_converted_slps_temp_impervious_R_V061_8day/SMUrF_TP39_fluxes.nc')\n",
    "#S_NEE_TP39_2019=g.variables['NEE'][:]\n",
    "#S_NEE_2019_std_TP39 = g.variables['NEE_std'][:]\n",
    "#S_GPP_TP39_2019=g.variables['GPP'][:]\n",
    "#S_GPP_2019_std_TP39 = g.variables['GPP_std'][:]\n",
    "#S_Reco_TP39_2019=g.variables['Reco'][:]\n",
    "#S_Reco_2019_std_TP39 = g.variables['Reco_std'][:]\n",
    "#S_time=g.variables['time'][:]\n",
    "#g.close()\n",
    "\n",
    "#g = Dataset('E:/Research/SMUrF/output2019_500m_CSIF_to_TROPOMI_CSIF_ALL_converted_slps_temp_impervious_R_V061_8day/SMUrF_TPD_fluxes_2019.nc')\n",
    "#S_NEE_TPD_2019=g.variables['NEE'][:]\n",
    "#S_NEE_2019_std_TPD = g.variables['NEE_std'][:]\n",
    "#S_GPP_TPD_2019=g.variables['GPP'][:]\n",
    "#S_GPP_2019_std_TPD = g.variables['GPP_std'][:]\n",
    "#S_Reco_TPD_2019=g.variables['Reco'][:]\n",
    "#S_Reco_2019_std_TPD = g.variables['Reco_std'][:]\n",
    "#S_time=g.variables['time'][:]\n",
    "#g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the spring (March, April, May) data\n",
    "#MAM: Day of year 60 - 151 inclusive\n",
    "\n",
    "with np.errstate(invalid='ignore'):\n",
    "    MAM_time=C_time_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    Borden_GPPgf_MAM=Borden_GEPgf[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_GPP_Borden_MAM=C_GPP_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_GPP_Borden_MAM=S_GPP_Borden[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "\n",
    "    Borden_Rgf_MAM=Borden_Rgf[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_Reco_Borden_MAM=C_Reco_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_Reco_Borden_MAM=S_Reco_Borden[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "\n",
    "    Borden_NEEgf_MAM=Borden_NEEgf[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    Borden_NEE_MAM=Borden_NEE[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_NEE_Borden_MAM=C_NEE_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_NEE_Borden_MAM=S_NEE_Borden[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 2019 Original SMUrF (using CSIF)\n",
    "\n",
    "# *** CHANGE PATH & FILENAME ***\n",
    "g=Dataset('E:/Research/SMUrF/output2019_CSIF_V061/easternCONUS/daily_mean_Reco_neuralnet/era5/2019/daily_mean_Reco_uncert_easternCONUS_20190101.nc')\n",
    "start_of_year_2019=g.variables['time'][0]/3600/24-1 #convert seconds since 1970 to days (minus one)\n",
    "g.close()\n",
    "\n",
    "# *** CHANGE PATH ***\n",
    "C_path = 'E:/Research/SMUrF/output2019_CSIF_V061/easternCONUS/hourly_flux_era5/'\n",
    "# *** CHANGE FILENAME ***\n",
    "C_fn = 'hrly_mean_GPP_Reco_NEE_easternCONUS_2019' #filename (without month)\n",
    "\n",
    "C_time_2019=[]\n",
    "C_Reco_2019=[]\n",
    "C_NEE_2019=[]\n",
    "C_GPP_2019=[]\n",
    "C_lats_2019=[]\n",
    "C_lons_2019=[]\n",
    "for j in range(1,13):\n",
    "    try:\n",
    "        if j<10:\n",
    "            # 2019:\n",
    "            f=Dataset(C_path+C_fn+'0'+str(j)+'.nc')\n",
    "        else:\n",
    "            # 2019:\n",
    "            f=Dataset(C_path+C_fn+str(j)+'.nc')\n",
    "        if len(C_time_2019)==0:\n",
    "            C_lats_2019=f.variables['lat'][:]\n",
    "            C_lons_2019=f.variables['lon'][:]\n",
    "            C_Reco_2019=f.variables['Reco_mean'][:]\n",
    "            C_GPP_2019=f.variables['GPP_mean'][:]\n",
    "            C_NEE_2019=f.variables['NEE_mean'][:]\n",
    "            C_time_2019=f.variables['time'][:]/24/3600-start_of_year_2019-5/24 #convert seconds since 1970 to days and subtract start of year and adjust to local time\n",
    "        else:\n",
    "            C_Reco_2019=np.concatenate((C_Reco_2019,f.variables['Reco_mean'][:]),axis=0)\n",
    "            C_GPP_2019=np.concatenate((C_GPP_2019,f.variables['GPP_mean'][:]),axis=0)\n",
    "            C_NEE_2019=np.concatenate((C_NEE_2019,f.variables['NEE_mean'][:]),axis=0)\n",
    "            C_time_2019=np.concatenate((C_time_2019,(f.variables['time'][:]/24/3600-start_of_year_2019-5/24)),axis=0)\n",
    "        f.close()\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "C_Reco_2019[C_Reco_2019==-999]=np.nan\n",
    "C_NEE_2019[C_NEE_2019==-999]=np.nan\n",
    "C_GPP_2019[C_GPP_2019==-999]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crop CSIF-SMUrF data over TP39\n",
    "\n",
    "C_GPP_TP39_array=np.zeros(8765)*np.nan\n",
    "C_NEE_TP39_array=np.zeros(8765)*np.nan\n",
    "C_Reco_TP39_array=np.zeros(8765)*np.nan\n",
    "\n",
    "for i in range(len(C_GPP[:,0,0])):\n",
    "    C_GPP_TP39_array[i]=C_GPP[i,4,6]\n",
    "    C_NEE_TP39_array[i]=C_NEE[i,4,6]\n",
    "    C_Reco_TP39_array[i]=C_Reco[i,4,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 2018 TP39 flux tower data & average to hourly resolution\n",
    "\n",
    "# *** CHANGE PATH & FILENAME ***\n",
    "TP39_2018_data=pd.read_csv('C:/Users/kitty/Documents/Research/SIF/Flux_Tower/Turkey_Point/TP39_HH_2018.csv', usecols=[0,1,2,6,77,78,79]) #header=1\n",
    "\n",
    "TP39_2018_dates=np.zeros([17520])*np.nan\n",
    "TP39_2018_NEE=np.zeros([17520])*np.nan #gapfilled NEE\n",
    "TP39_2018_NEE2=np.zeros([17520])*np.nan #non-gapfilled NEE\n",
    "TP39_2018_GPP=np.zeros([17520])*np.nan\n",
    "TP39_2018_R=np.zeros([17520])*np.nan\n",
    "n=0\n",
    "m=0\n",
    "#date=1\n",
    "for i in range(17520):\n",
    "    if 201801010000<=TP39_2018_data.iat[i,0]<202001010000:\n",
    "        TP39_2018_dates[i]=datetime.strptime(str(int(TP39_2018_data.iat[i,0])),'%Y%m%d%H%M').timetuple().tm_yday+(float(str(TP39_2018_data.iat[i,0])[8:10])+float(str(TP39_2018_data.iat[i,0])[10:12])/60)/24\n",
    "        #check that the value is greater than -9999 (value for empty measurements)\n",
    "        if TP39_2018_data.iat[i,2]>-9999:\n",
    "            TP39_2018_NEE2[i]=TP39_2018_data.iat[i,2] # save the non-gapfilled NEE value\n",
    "        if TP39_2018_data.iat[i,6]>-9999:\n",
    "            TP39_2018_NEE[i]=TP39_2018_data.iat[i,6] # save the gapfilled NEE value\n",
    "        if TP39_2018_data.iat[i,4]>-9999:\n",
    "            TP39_2018_GPP[i]=TP39_2018_data.iat[i,4] # save the GPP value\n",
    "        if TP39_2018_data.iat[i,5]>-9999:\n",
    "            TP39_2018_R[i]=TP39_2018_data.iat[i,5] # save the Reco value\n",
    "\n",
    "TP39_GPP=np.zeros(np.shape(C_GPP_TP39_array))*np.nan\n",
    "TP39_NEEgf=np.zeros(np.shape(C_GPP_TP39_array))*np.nan\n",
    "TP39_NEE=np.zeros(np.shape(C_GPP_TP39_array))*np.nan\n",
    "TP39_R=np.zeros(np.shape(C_GPP_TP39_array))*np.nan\n",
    "for i in range(len(C_time_array)-5):\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        TP39_GPP[i+5]=np.nanmean([TP39_2018_GPP[i*2],TP39_2018_GPP[i*2+1]])\n",
    "        TP39_NEEgf[i+5]=np.nanmean([TP39_2018_NEE[i*2],TP39_2018_NEE[i*2+1]])\n",
    "        TP39_NEE[i+5]=np.nanmean([TP39_2018_NEE2[i*2],TP39_2018_NEE2[i*2+1]])\n",
    "        TP39_R[i+5]=np.nanmean([TP39_2018_R[i*2],TP39_2018_R[i*2+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select spring 2018 TP39 data\n",
    "\n",
    "#MAM: Doy 60 - 151 inclusive\n",
    "\n",
    "with np.errstate(invalid='ignore'):\n",
    "    TP39_GPP_MAM=TP39_GPP[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_GPP_TP39_MAM=C_GPP_TP39_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_GPP_TP39_MAM=S_GPP_TP39[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "\n",
    "    TP39_R_MAM=TP39_R[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_Reco_TP39_MAM=C_Reco_TP39_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_Reco_TP39_MAM=S_Reco_TP39[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "\n",
    "    TP39_NEEgf_MAM=TP39_NEEgf[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    TP39_NEE_MAM=TP39_NEE[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_NEE_TP39_MAM=C_NEE_TP39_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_NEE_TP39_MAM=S_NEE_TP39[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in 2019 TP39 data & average to hourly resolution\n",
    "\n",
    "# *** CHANGE PATH & FILENAME ***\n",
    "TP39_2019_data=pd.read_csv('C:/Users/kitty/Documents/Research/SIF/Flux_Tower/Turkey_Point/TP39_HH_2019.csv', usecols=[0,1,2,6,77,78,79]) #header=1\n",
    "\n",
    "TP39_2019_dates=np.zeros([17520])*np.nan\n",
    "TP39_2019_dates2=np.zeros([17520])*np.nan\n",
    "TP39_2019_NEE=np.zeros([17520])*np.nan\n",
    "TP39_2019_NEE2=np.zeros([17520])*np.nan\n",
    "TP39_2019_GPP=np.zeros([17520])*np.nan\n",
    "TP39_2019_R=np.zeros([17520])*np.nan\n",
    "n=0\n",
    "m=0\n",
    "date=1\n",
    "for i in range(17520):\n",
    "    if 201901010000<=TP39_2019_data.iat[i,0]<202001010000:\n",
    "        TP39_2019_dates[i]=datetime.strptime(str(int(TP39_2019_data.iat[i,0])),'%Y%m%d%H%M').timetuple().tm_yday+(float(str(TP39_2019_data.iat[i,0])[8:10])+float(str(TP39_2019_data.iat[i,0])[10:12])/60)/24\n",
    "\n",
    "        #check that the value is greater than -9999 (value for empty measurements)\n",
    "        if TP39_2019_data.iat[i,2]>-9999:\n",
    "            TP39_2019_NEE2[i]=TP39_2019_data.iat[i,2] # save the non-gapfilled NEE value\n",
    "        if TP39_2019_data.iat[i,6]>-9999:\n",
    "            TP39_2019_NEE[i]=TP39_2019_data.iat[i,6] # save the gapfilled NEE value\n",
    "        if TP39_2019_data.iat[i,4]>-9999:\n",
    "            TP39_2019_GPP[i]=TP39_2019_data.iat[i,4] # save the GPP value\n",
    "        if TP39_2019_data.iat[i,5]>-9999:\n",
    "            TP39_2019_R[i]=TP39_2019_data.iat[i,5] # save the Reco value\n",
    "        \n",
    "# Select Original SMUrF data over TP39\n",
    "C_GPP_TP39_2019_array=np.zeros(8765)*np.nan\n",
    "C_NEE_TP39_2019_array=np.zeros(8765)*np.nan\n",
    "C_Reco_TP39_2019_array=np.zeros(8765)*np.nan\n",
    "C_time_2019_array=np.zeros(8765)*np.nan\n",
    "for i in range(len(C_GPP_2019[:,0,0])):\n",
    "    C_time_2019_array[i]=C_time[i]\n",
    "    C_GPP_TP39_2019_array[i]=C_GPP_2019[i,4,6]\n",
    "    C_NEE_TP39_2019_array[i]=C_NEE_2019[i,4,6]\n",
    "    C_Reco_TP39_2019_array[i]=C_Reco_2019[i,4,6]\n",
    "\n",
    "TP39_2019_hrly_GPP=np.zeros(np.shape(C_GPP_TP39_2019_array))*np.nan\n",
    "TP39_2019_hrly_NEEgf=np.zeros(np.shape(C_GPP_TP39_2019_array))*np.nan\n",
    "TP39_2019_hrly_NEE=np.zeros(np.shape(C_GPP_TP39_2019_array))*np.nan\n",
    "TP39_2019_hrly_R=np.zeros(np.shape(C_GPP_TP39_2019_array))*np.nan\n",
    "for i in range(len(C_time_array)-5):\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        TP39_2019_hrly_GPP[i+5]=np.nanmean([TP39_2019_GPP[i*2],TP39_2019_GPP[i*2+1]])\n",
    "        TP39_2019_hrly_NEEgf[i+5]=np.nanmean([TP39_2019_NEE[i*2],TP39_2019_NEE[i*2+1]])\n",
    "        TP39_2019_hrly_NEE[i+5]=np.nanmean([TP39_2019_NEE2[i*2],TP39_2019_NEE2[i*2+1]])\n",
    "        TP39_2019_hrly_R[i+5]=np.nanmean([TP39_2019_R[i*2],TP39_2019_R[i*2+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out erroneous NEE values between doy 195 and 198\n",
    "\n",
    "## ***Optional: uncomment to visualize erroneous values\n",
    "#with np.errstate(invalid='ignore'):\n",
    "#    plt.figure()\n",
    "#    plt.xlim(193,200)\n",
    "#    plt.scatter(C_time_array,TP39_2019_hrly_NEE,label='TP39 NEE')\n",
    "#    plt.scatter(C_time_array[(C_time_array>195.05-5/24) & (C_time_array<198.6-5/24)],TP39_2019_hrly_NEE[(C_time_array>195.05-5/24) & (C_time_array<198.6-5/24)],label='Erroneous TP39 NEE')\n",
    "#    plt.scatter(C_time_array,S_NEE_TP39_2019,marker='*',label='SMUrF NEE')\n",
    "#    plt.legend()\n",
    "#    plt.xlabel('Day of year, 2019')\n",
    "#    plt.ylabel('NEE ($\\mu$mol m$^{-2}$ s$^{-1}$)')\n",
    "#    plt.title('Erroneous TP39 flux tower NEE values')\n",
    "#    plt.show()\n",
    "# ***\n",
    "\n",
    "with np.errstate(invalid='ignore'):\n",
    "    TP39_2019_hrly_NEE[(C_time_array>195.05-5/24) & (C_time_array<198.6-5/24)]= np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select spring data over TP39\n",
    "\n",
    "##MAM: Doy 60 - 151 inclusive\n",
    "with np.errstate(invalid='ignore'):\n",
    "    TP39_2019_GPP_MAM=TP39_2019_hrly_GPP[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_GPP_TP39_2019_MAM=C_GPP_TP39_2019_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_GPP_TP39_2019_MAM=S_GPP_TP39_2019[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "\n",
    "    TP39_2019_R_MAM=TP39_2019_hrly_R[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_Reco_TP39_2019_MAM=C_Reco_TP39_2019_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_Reco_TP39_2019_MAM=S_Reco_TP39_2019[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "\n",
    "    TP39_2019_NEEgf_MAM=TP39_2019_hrly_NEEgf[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    TP39_2019_NEE_MAM=TP39_2019_hrly_NEE[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_NEE_TP39_2019_MAM=C_NEE_TP39_2019_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_NEE_TP39_2019_MAM=S_NEE_TP39_2019[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in TPD 2018 flux tower data & average to hourly resolution\n",
    "\n",
    "# *** CHANGE PATH & FILENAME ***\n",
    "TPD_2018_data=pd.read_csv('C:/Users/kitty/Documents/Research/SIF/Flux_Tower/Turkey_Point/TPD_HH_2018.csv', usecols=[0,1,2,6,74,75,76]) #header=1\n",
    "\n",
    "TPD_2018_dates=np.zeros([17520])*np.nan\n",
    "TPD_2018_dates2=np.zeros([17520])*np.nan\n",
    "TPD_2018_NEE=np.zeros([17520])*np.nan\n",
    "TPD_2018_NEE2=np.zeros([17520])*np.nan\n",
    "TPD_2018_GPP=np.zeros([17520])*np.nan\n",
    "TPD_2018_R=np.zeros([17520])*np.nan\n",
    "n=0\n",
    "m=0\n",
    "date=1\n",
    "for i in range(17520):\n",
    "    if 201801010000<=TPD_2018_data.iat[i,0]<202001010000:\n",
    "        TPD_2018_dates[i]=datetime.strptime(str(int(TPD_2018_data.iat[i,0])),'%Y%m%d%H%M').timetuple().tm_yday+(float(str(TPD_2018_data.iat[i,0])[8:10])+float(str(TPD_2018_data.iat[i,0])[10:12])/60)/24 #save the current date (and time)\n",
    "        #check that the value is greater than -9999 (value for empty measurements)\n",
    "        if TPD_2018_data.iat[i,2]>-9999:\n",
    "            TPD_2018_NEE2[i]=TPD_2018_data.iat[i,2] # save the NEE value\n",
    "        if TPD_2018_data.iat[i,6]>-9999:\n",
    "            TPD_2018_NEE[i]=TPD_2018_data.iat[i,6] # save the NEE value\n",
    "        if TPD_2018_data.iat[i,4]>-9999:\n",
    "            TPD_2018_GPP[i]=TPD_2018_data.iat[i,4] # save the NEE value\n",
    "        if TPD_2018_data.iat[i,5]>-9999:\n",
    "            TPD_2018_R[i]=TPD_2018_data.iat[i,5] # save the NEE value\n",
    "\n",
    "# Select Original SMUrF data over TPD\n",
    "C_GPP_TPD_array=np.zeros(8765)*np.nan\n",
    "C_NEE_TPD_array=np.zeros(8765)*np.nan\n",
    "C_Reco_TPD_array=np.zeros(8765)*np.nan\n",
    "for i in range(len(C_GPP[:,0,0])):\n",
    "    C_GPP_TPD_array[i]=np.nanmean([C_GPP[i,2,2]])\n",
    "    C_NEE_TPD_array[i]=np.nanmean([C_NEE[i,2,2]])\n",
    "    C_Reco_TPD_array[i]=np.nanmean([C_Reco[i,2,2]])\n",
    "\n",
    "# Average TPD fluxtower data to hourly resolution\n",
    "TPD_GPP=np.zeros(np.shape(C_GPP_TPD_array))*np.nan\n",
    "TPD_NEE=np.zeros(np.shape(C_GPP_TPD_array))*np.nan\n",
    "TPD_NEEgf=np.zeros(np.shape(C_GPP_TPD_array))*np.nan\n",
    "TPD_R=np.zeros(np.shape(C_GPP_TPD_array))*np.nan\n",
    "for i in range(len(C_time_array)-5):\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        TPD_GPP[i+5]=np.nanmean([TPD_2018_GPP[i*2],TPD_2018_GPP[i*2+1]])\n",
    "        TPD_NEE[i+5]=np.nanmean([TPD_2018_NEE2[i*2],TPD_2018_NEE2[i*2+1]])\n",
    "        TPD_NEEgf[i+5]=np.nanmean([TPD_2018_NEE[i*2],TPD_2018_NEE[i*2+1]])\n",
    "        TPD_R[i+5]=np.nanmean([TPD_2018_R[i*2],TPD_2018_R[i*2+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select spring (March-May) 2018 data over TPD\n",
    "#MAM: Doy 60 - 151 inclusive\n",
    "\n",
    "with np.errstate(invalid='ignore'):\n",
    "    TPD_GPP_MAM=TPD_GPP[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_GPP_TPD_MAM=C_GPP_TPD_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_GPP_TPD_MAM=S_GPP_TPD[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "\n",
    "    TPD_R_MAM=TPD_R[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_Reco_TPD_MAM=C_Reco_TPD_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_Reco_TPD_MAM=S_Reco_TPD[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "\n",
    "    TPD_NEEgf_MAM=TPD_NEEgf[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    TPD_NEE_MAM=TPD_NEE[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_NEE_TPD_MAM=C_NEE_TPD_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_NEE_TPD_MAM=S_NEE_TPD[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in 2019 TPD fluxtower data\n",
    "\n",
    "# *** CHANGE PATH AND FILENAME ***\n",
    "TPD_2019_data=pd.read_csv('C:/Users/kitty/Documents/Research/SIF/Flux_Tower/Turkey_Point/TPD_HH_2019.csv', usecols=[0,1,2,6,74,75,76]) #header=1\n",
    "\n",
    "TPD_2019_dates=np.zeros([17520])*np.nan\n",
    "TPD_2019_dates2=np.zeros([17520])*np.nan\n",
    "TPD_2019_NEE=np.zeros([17520])*np.nan\n",
    "TPD_2019_NEE2=np.zeros([17520])*np.nan\n",
    "TPD_2019_GPP=np.zeros([17520])*np.nan\n",
    "TPD_2019_R=np.zeros([17520])*np.nan\n",
    "n=0\n",
    "m=0\n",
    "date=1\n",
    "for i in range(17520):\n",
    "    if 201901010000<=TPD_2019_data.iat[i,0]<202001010000:\n",
    "        TPD_2019_dates[i]=TPD_2019_data.iat[i,0] #save the current date (and time)\n",
    "        TPD_2019_dates2[i]=date+n/48\n",
    "        #check that the value is greater than -9999 (value for empty measurements)\n",
    "        if TPD_2019_data.iat[i,2]>-9999:\n",
    "            TPD_2019_NEE2[i]=TPD_2019_data.iat[i,2] # save the NEE value\n",
    "        if TPD_2019_data.iat[i,6]>-9999:\n",
    "            TPD_2019_NEE[i]=TPD_2019_data.iat[i,6] # save the NEE value\n",
    "        if TPD_2019_data.iat[i,4]>-9999:\n",
    "            TPD_2019_GPP[i]=TPD_2019_data.iat[i,4] # save the NEE value\n",
    "        if TPD_2019_data.iat[i,5]>-9999:\n",
    "            TPD_2019_R[i]=TPD_2019_data.iat[i,5] # save the NEE value\n",
    "\n",
    "#Select original SMUrF pixel over TPD\n",
    "C_GPP_TPD_2019_array=np.zeros(8765)*np.nan\n",
    "C_NEE_TPD_2019_array=np.zeros(8765)*np.nan\n",
    "C_Reco_TPD_2019_array=np.zeros(8765)*np.nan\n",
    "\n",
    "for i in range(len(C_GPP_2019[:,0,0])):\n",
    "    C_GPP_TPD_2019_array[i]=np.nanmean([C_GPP_2019[i,2,2]])\n",
    "    C_NEE_TPD_2019_array[i]=np.nanmean([C_NEE_2019[i,2,2]])\n",
    "    C_Reco_TPD_2019_array[i]=np.nanmean([C_Reco_2019[i,2,2]])\n",
    "\n",
    "#Average 30-minute fluxtower data to hourly resolution:\n",
    "TPD_2019_hrly_GPP=np.zeros(np.shape(C_GPP_TPD_2019_array))*np.nan\n",
    "TPD_2019_hrly_NEE=np.zeros(np.shape(C_GPP_TPD_2019_array))*np.nan\n",
    "TPD_2019_hrly_NEEgf=np.zeros(np.shape(C_GPP_TPD_2019_array))*np.nan\n",
    "TPD_2019_hrly_R=np.zeros(np.shape(C_GPP_TPD_2019_array))*np.nan\n",
    "\n",
    "for i in range(len(C_time_array)-6):\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        TPD_2019_hrly_GPP[i+5]=np.nanmean([TPD_2019_GPP[i*2],TPD_2019_GPP[i*2+1]])\n",
    "        TPD_2019_hrly_NEE[i+5]=np.nanmean([TPD_2019_NEE2[i*2],TPD_2019_NEE2[i*2+1]])\n",
    "        TPD_2019_hrly_NEEgf[i+5]=np.nanmean([TPD_2019_NEE[i*2],TPD_2019_NEE[i*2+1]])\n",
    "        TPD_2019_hrly_R[i+5]=np.nanmean([TPD_2019_R[i*2],TPD_2019_R[i*2+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only spring data over TPD in 2019\n",
    "#MAM: Doy 60 - 151 inclusive\n",
    "\n",
    "with np.errstate(invalid='ignore'):\n",
    "    TPD_2019_GPP_MAM=TPD_2019_hrly_GPP[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_GPP_TPD_2019_MAM=C_GPP_TPD_2019_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_GPP_TPD_2019_MAM=S_GPP_TPD_2019[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "\n",
    "    TPD_2019_R_MAM=TPD_2019_hrly_R[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_Reco_TPD_2019_MAM=C_Reco_TPD_2019_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_Reco_TPD_2019_MAM=S_Reco_TPD_2019[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "\n",
    "    TPD_2019_NEEgf_MAM=TPD_2019_hrly_NEEgf[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    TPD_2019_NEE_MAM=TPD_2019_hrly_NEE[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    C_NEE_TPD_2019_MAM=C_NEE_TPD_2019_array[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]\n",
    "    S_NEE_TPD_2019_MAM=S_NEE_TPD_2019[(np.round(C_time_array,5)>=60) & (np.round(C_time_array,5)<152)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data over all fluxtowers in spring\n",
    "Fluxtower_GPP_MAM_tot = np.concatenate([Borden_GPPgf_MAM,TP39_GPP_MAM,TPD_GPP_MAM,TP39_2019_GPP_MAM,TPD_2019_GPP_MAM])\n",
    "C_GPP_MAM_tot = np.concatenate([C_GPP_Borden_MAM,C_GPP_TP39_MAM,C_GPP_TPD_MAM,C_GPP_TP39_2019_MAM,C_GPP_TPD_2019_MAM])\n",
    "S_GPP_MAM_tot = np.concatenate([S_GPP_Borden_MAM,S_GPP_TP39_MAM,S_GPP_TPD_MAM,S_GPP_TP39_2019_MAM,S_GPP_TPD_2019_MAM])\n",
    "\n",
    "Fluxtower_Reco_MAM_tot = np.concatenate([Borden_Rgf_MAM,TP39_R_MAM,TPD_R_MAM,TP39_2019_R_MAM,TPD_2019_R_MAM])\n",
    "C_Reco_MAM_tot = np.concatenate([C_Reco_Borden_MAM,C_Reco_TP39_MAM,C_Reco_TPD_MAM,C_Reco_TP39_2019_MAM,C_Reco_TPD_2019_MAM])\n",
    "S_Reco_MAM_tot = np.concatenate([S_Reco_Borden_MAM,S_Reco_TP39_MAM,S_Reco_TPD_MAM,S_Reco_TP39_2019_MAM,S_Reco_TPD_2019_MAM])\n",
    "\n",
    "Fluxtower_NEEgf_MAM_tot = np.concatenate([Borden_NEEgf_MAM,TP39_NEEgf_MAM,TPD_NEEgf_MAM,TP39_2019_NEEgf_MAM,TPD_2019_NEEgf_MAM])\n",
    "Fluxtower_NEE_MAM_tot = np.concatenate([Borden_NEE_MAM,TP39_NEE_MAM,TPD_NEE_MAM,TP39_2019_NEE_MAM,TPD_2019_NEE_MAM])\n",
    "C_NEE_MAM_tot = np.concatenate([C_NEE_Borden_MAM,C_NEE_TP39_MAM,C_NEE_TPD_MAM,C_NEE_TP39_2019_MAM,C_NEE_TPD_2019_MAM])\n",
    "S_NEE_MAM_tot = np.concatenate([S_NEE_Borden_MAM,S_NEE_TP39_MAM,S_NEE_TPD_MAM,S_NEE_TP39_2019_MAM,S_NEE_TPD_2019_MAM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define linear function for plotting\n",
    "line1_1=np.arange(-100,100)\n",
    "\n",
    "def func2(x,m,b):\n",
    "    return m*x+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit original SMUrF data to fluxtower data using a bootstrapped Huber fit\n",
    "\n",
    "finitemask0 = np.isfinite(Fluxtower_NEE_MAM_tot)\n",
    "Fluxtower_NEE_MAMclean0 = Fluxtower_NEE_MAM_tot[finitemask0]\n",
    "C_NEE_MAMclean0 = C_NEE_MAM_tot[finitemask0]\n",
    "\n",
    "finitemask2 = np.isfinite(C_NEE_MAMclean0)\n",
    "C_NEE_MAMclean1 = C_NEE_MAMclean0[finitemask2]\n",
    "Fluxtower_NEE_MAMclean1 = Fluxtower_NEE_MAMclean0[finitemask2]\n",
    "\n",
    "Huber_tot_MAM_C_NEE_slps=[]\n",
    "Huber_tot_MAM_C_NEE_ints=[]\n",
    "Huber_tot_MAM_C_NEE_R2=[]\n",
    "\n",
    "#try bootstrapping\n",
    "indx_list=list(range(0,len(C_NEE_MAMclean1)))\n",
    "for i in range(1,1001):\n",
    "    #sub selection of points\n",
    "    NEE_indx=np.random.choice(indx_list,size=len(C_NEE_MAMclean1))\n",
    "\n",
    "    Huber_model = linear_model.HuberRegressor(fit_intercept=True)\n",
    "    Huber_fit=Huber_model.fit((Fluxtower_NEE_MAMclean1[NEE_indx]).reshape(-1,1),C_NEE_MAMclean1[NEE_indx])\n",
    "    H_m=Huber_fit.coef_\n",
    "    H_c=Huber_fit.intercept_\n",
    "    x_accpt, y_accpt = Fluxtower_NEE_MAMclean1, C_NEE_MAMclean1\n",
    "    y_predict = H_m * x_accpt + H_c\n",
    "    H_R2=r2_score(y_accpt, y_predict)\n",
    "    Huber_tot_MAM_C_NEE_slps.append(H_m)\n",
    "    Huber_tot_MAM_C_NEE_ints.append(H_c)\n",
    "    Huber_tot_MAM_C_NEE_R2.append(H_R2)\n",
    "    \n",
    "y_predict = np.nanmean(Huber_tot_MAM_C_NEE_slps) * x_accpt + np.nanmean(Huber_tot_MAM_C_NEE_ints)\n",
    "Huber_MAM_C_NEE_R2=r2_score(y_accpt, y_predict)\n",
    "\n",
    "print('Original SMUrF MAM slope: '+str(np.round(np.nanmean(Huber_tot_MAM_C_NEE_slps),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_MAM_C_NEE_slps),3)))\n",
    "print('Original SMUrF MAM intercept: '+str(np.round(np.nanmean(Huber_tot_MAM_C_NEE_ints),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_MAM_C_NEE_ints),3)))\n",
    "\n",
    "print('Original SMUrF MAM R^2: '+str(np.round(np.nanmean(Huber_MAM_C_NEE_R2),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Modified SMUrF data to fluxtower data using a bootstrapped Huber fit\n",
    "          \n",
    "finitemask0 = np.isfinite(Fluxtower_NEE_MAM_tot)\n",
    "Fluxtower_NEE_MAMclean0 = Fluxtower_NEE_MAM_tot[finitemask0]\n",
    "S_NEE_MAMclean0 = S_NEE_MAM_tot[finitemask0]\n",
    "\n",
    "finitemask2 = np.isfinite(S_NEE_MAMclean0)\n",
    "S_NEE_MAMclean1 = S_NEE_MAMclean0[finitemask2]\n",
    "Fluxtower_NEE_MAMclean1 = Fluxtower_NEE_MAMclean0[finitemask2]\n",
    "\n",
    "Huber_tot_MAM_S_NEE_slps=[]\n",
    "Huber_tot_MAM_S_NEE_ints=[]\n",
    "Huber_tot_MAM_S_NEE_R2=[]\n",
    "\n",
    "#try bootstrapping\n",
    "indx_list=list(range(0,len(S_NEE_MAMclean1)))\n",
    "for i in range(1,1001):\n",
    "    #sub selection of points\n",
    "    NEE_indx=np.random.choice(indx_list,size=len(S_NEE_MAMclean1))\n",
    "\n",
    "    Huber_model = linear_model.HuberRegressor(fit_intercept=True)\n",
    "    Huber_fit=Huber_model.fit((Fluxtower_NEE_MAMclean1[NEE_indx]).reshape(-1,1),S_NEE_MAMclean1[NEE_indx])\n",
    "    H_m=Huber_fit.coef_\n",
    "    H_c=Huber_fit.intercept_\n",
    "    x_accpt, y_accpt = Fluxtower_NEE_MAMclean1, S_NEE_MAMclean1\n",
    "    y_predict = H_m * x_accpt + H_c\n",
    "    H_R2=r2_score(y_accpt, y_predict)\n",
    "    Huber_tot_MAM_S_NEE_slps.append(H_m)\n",
    "    Huber_tot_MAM_S_NEE_ints.append(H_c)\n",
    "    Huber_tot_MAM_S_NEE_R2.append(H_R2)\n",
    "    \n",
    "#print(np.nanmean(Huber_tot_MAM_S_NEE_slps),np.nanstd(Huber_tot_MAM_S_NEE_slps),np.nanmean(Huber_tot_MAM_S_NEE_ints),np.nanstd(Huber_tot_MAM_S_NEE_ints),np.nanmean(Huber_tot_MAM_S_NEE_R2),np.nanstd(Huber_tot_MAM_S_NEE_R2))\n",
    "\n",
    "x_accpt, y_accpt = Fluxtower_NEE_MAMclean1, S_NEE_MAMclean1\n",
    "y_predict = np.nanmean(Huber_tot_MAM_S_NEE_slps) * x_accpt + np.nanmean(Huber_tot_MAM_S_NEE_ints)\n",
    "Huber_MAM_S_NEE_R2=r2_score(y_accpt, y_predict)\n",
    "#print(Huber_MAM_S_NEE_R2)\n",
    "\n",
    "print('Updated SMUrF MAM slope: '+str(np.round(np.nanmean(Huber_tot_MAM_S_NEE_slps),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_MAM_S_NEE_slps),3)))\n",
    "print('Updated SMUrF MAM intercept: '+str(np.round(np.nanmean(Huber_tot_MAM_S_NEE_ints),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_MAM_S_NEE_ints),3)))\n",
    "\n",
    "print('Updated SMUrF MAM R^2: '+str(np.round(np.nanmean(Huber_MAM_S_NEE_R2),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fluxes over towers for summer\n",
    "\n",
    "#JJA: Doy 152 - 223 inclusive\n",
    "with np.errstate(invalid='ignore'):\n",
    "    JJA_time=C_time_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    Borden_GPPgf_JJA=Borden_GEPgf[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_GPP_Borden_JJA=C_GPP_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_GPP_Borden_JJA=S_GPP_Borden[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    Borden_Rgf_JJA=Borden_Rgf[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_Reco_Borden_JJA=C_Reco_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_Reco_Borden_JJA=S_Reco_Borden[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    Borden_NEEgf_JJA=Borden_NEEgf[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    Borden_NEE_JJA=Borden_NEE[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_NEE_Borden_JJA=C_NEE_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_NEE_Borden_JJA=S_NEE_Borden[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    TP39_GPP_JJA=TP39_GPP[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_GPP_TP39_JJA=C_GPP_TP39_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_GPP_TP39_JJA=S_GPP_TP39[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    TP39_R_JJA=TP39_R[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_Reco_TP39_JJA=C_Reco_TP39_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_Reco_TP39_JJA=S_Reco_TP39[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    TP39_NEEgf_JJA=TP39_NEEgf[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    TP39_NEE_JJA=TP39_NEE[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_NEE_TP39_JJA=C_NEE_TP39_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_NEE_TP39_JJA=S_NEE_TP39[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    TP39_2019_GPP_JJA=TP39_2019_hrly_GPP[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_GPP_TP39_2019_JJA=C_GPP_TP39_2019_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_GPP_TP39_2019_JJA=S_GPP_TP39_2019[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    TP39_2019_R_JJA=TP39_2019_hrly_R[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_Reco_TP39_2019_JJA=C_Reco_TP39_2019_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_Reco_TP39_2019_JJA=S_Reco_TP39_2019[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    TP39_2019_NEEgf_JJA=TP39_2019_hrly_NEEgf[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    TP39_2019_NEE_JJA=TP39_2019_hrly_NEE[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_NEE_TP39_2019_JJA=C_NEE_TP39_2019_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_NEE_TP39_2019_JJA=S_NEE_TP39_2019[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    TPD_GPP_JJA=TPD_GPP[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_GPP_TPD_JJA=C_GPP_TPD_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_GPP_TPD_JJA=S_GPP_TPD[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    TPD_R_JJA=TPD_R[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_Reco_TPD_JJA=C_Reco_TPD_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_Reco_TPD_JJA=S_Reco_TPD[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    TPD_NEEgf_JJA=TPD_NEEgf[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    TPD_NEE_JJA=TPD_NEE[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_NEE_TPD_JJA=C_NEE_TPD_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_NEE_TPD_JJA=S_NEE_TPD[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    TPD_2019_GPP_JJA=TPD_2019_hrly_GPP[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_GPP_TPD_2019_JJA=C_GPP_TPD_2019_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_GPP_TPD_2019_JJA=S_GPP_TPD_2019[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    TPD_2019_R_JJA=TPD_2019_hrly_R[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_Reco_TPD_2019_JJA=C_Reco_TPD_2019_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_Reco_TPD_2019_JJA=S_Reco_TPD_2019[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "\n",
    "    TPD_2019_NEEgf_JJA=TPD_2019_hrly_NEEgf[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    TPD_2019_NEE_JJA=TPD_2019_hrly_NEE[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    C_NEE_TPD_2019_JJA=C_NEE_TPD_2019_array[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]\n",
    "    S_NEE_TPD_2019_JJA=S_NEE_TPD_2019[(np.round(C_time_array,5)>=152) & (np.round(C_time_array,5)<224)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data over all fluxtowers in summer\n",
    "Fluxtower_GPP_JJA_tot = np.concatenate([Borden_GPPgf_JJA,TP39_GPP_JJA,TPD_GPP_JJA,TP39_2019_GPP_JJA,TPD_2019_GPP_JJA])\n",
    "C_GPP_JJA_tot = np.concatenate([C_GPP_Borden_JJA,C_GPP_TP39_JJA,C_GPP_TPD_JJA,C_GPP_TP39_2019_JJA,C_GPP_TPD_2019_JJA])\n",
    "S_GPP_JJA_tot = np.concatenate([S_GPP_Borden_JJA,S_GPP_TP39_JJA,S_GPP_TPD_JJA,S_GPP_TP39_2019_JJA,S_GPP_TPD_2019_JJA])\n",
    "\n",
    "Fluxtower_Reco_JJA_tot = np.concatenate([Borden_Rgf_JJA,TP39_R_JJA,TPD_R_JJA,TP39_2019_R_JJA,TPD_2019_R_JJA])\n",
    "C_Reco_JJA_tot = np.concatenate([C_Reco_Borden_JJA,C_Reco_TP39_JJA,C_Reco_TPD_JJA,C_Reco_TP39_2019_JJA,C_Reco_TPD_2019_JJA])\n",
    "S_Reco_JJA_tot = np.concatenate([S_Reco_Borden_JJA,S_Reco_TP39_JJA,S_Reco_TPD_JJA,S_Reco_TP39_2019_JJA,S_Reco_TPD_2019_JJA])\n",
    "\n",
    "Fluxtower_NEEgf_JJA_tot = np.concatenate([Borden_NEEgf_JJA,TP39_NEEgf_JJA,TPD_NEEgf_JJA,TP39_2019_NEEgf_JJA,TPD_2019_NEEgf_JJA])\n",
    "Fluxtower_NEE_JJA_tot = np.concatenate([Borden_NEE_JJA,TP39_NEE_JJA,TPD_NEE_JJA,TP39_2019_NEE_JJA,TPD_2019_NEE_JJA])\n",
    "C_NEE_JJA_tot = np.concatenate([C_NEE_Borden_JJA,C_NEE_TP39_JJA,C_NEE_TPD_JJA,C_NEE_TP39_2019_JJA,C_NEE_TPD_2019_JJA])\n",
    "S_NEE_JJA_tot = np.concatenate([S_NEE_Borden_JJA,S_NEE_TP39_JJA,S_NEE_TPD_JJA,S_NEE_TP39_2019_JJA,S_NEE_TPD_2019_JJA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Original SMUrF data to fluxtower data using a bootstrapped Huber fit\n",
    "          \n",
    "finitemask0 = np.isfinite(Fluxtower_NEE_JJA_tot)\n",
    "Fluxtower_NEE_JJAclean0 = Fluxtower_NEE_JJA_tot[finitemask0]\n",
    "C_NEE_JJAclean0 = C_NEE_JJA_tot[finitemask0]\n",
    "\n",
    "finitemask2 = np.isfinite(C_NEE_JJAclean0)\n",
    "C_NEE_JJAclean1 = C_NEE_JJAclean0[finitemask2]\n",
    "Fluxtower_NEE_JJAclean1 = Fluxtower_NEE_JJAclean0[finitemask2]\n",
    "\n",
    "Huber_tot_JJA_C_NEE_slps=[]\n",
    "Huber_tot_JJA_C_NEE_ints=[]\n",
    "Huber_tot_JJA_C_NEE_R2=[]\n",
    "\n",
    "#try bootstrapping\n",
    "indx_list=list(range(0,len(C_NEE_JJAclean1)))\n",
    "for i in range(1,1001):\n",
    "    #sub selection of points\n",
    "    NEE_indx=np.random.choice(indx_list,size=len(C_NEE_JJAclean1))\n",
    "\n",
    "    Huber_model = linear_model.HuberRegressor(fit_intercept=True)\n",
    "    Huber_fit=Huber_model.fit((Fluxtower_NEE_JJAclean1[NEE_indx]).reshape(-1,1),C_NEE_JJAclean1[NEE_indx])\n",
    "    H_m=Huber_fit.coef_\n",
    "    H_c=Huber_fit.intercept_\n",
    "    x_accpt, y_accpt = Fluxtower_NEE_JJAclean1, C_NEE_JJAclean1\n",
    "    y_predict = H_m * x_accpt + H_c\n",
    "    H_R2=r2_score(y_accpt, y_predict)\n",
    "    Huber_tot_JJA_C_NEE_slps.append(H_m)\n",
    "    Huber_tot_JJA_C_NEE_ints.append(H_c)\n",
    "    Huber_tot_JJA_C_NEE_R2.append(H_R2)\n",
    "    \n",
    "y_predict = np.nanmean(Huber_tot_JJA_C_NEE_slps) * x_accpt + np.nanmean(Huber_tot_JJA_C_NEE_ints)\n",
    "Huber_JJA_C_NEE_R2=r2_score(y_accpt, y_predict)\n",
    "\n",
    "print('Original SMUrF JJA slope: '+str(np.round(np.nanmean(Huber_tot_JJA_C_NEE_slps),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_JJA_C_NEE_slps),3)))\n",
    "print('Original SMUrF JJA intercept: '+str(np.round(np.nanmean(Huber_tot_JJA_C_NEE_ints),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_JJA_C_NEE_ints),3)))\n",
    "\n",
    "print('Original SMUrF JJA R^2: '+str(np.round(np.nanmean(Huber_JJA_C_NEE_R2),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Modified SMUrF data to fluxtower data using bootstrapped Huber fit\n",
    "          \n",
    "finitemask0 = np.isfinite(Fluxtower_NEE_JJA_tot)\n",
    "Fluxtower_NEE_JJAclean0 = Fluxtower_NEE_JJA_tot[finitemask0]\n",
    "S_NEE_JJAclean0 = S_NEE_JJA_tot[finitemask0]\n",
    "\n",
    "finitemask2 = np.isfinite(S_NEE_JJAclean0)\n",
    "S_NEE_JJAclean1 = S_NEE_JJAclean0[finitemask2]\n",
    "Fluxtower_NEE_JJAclean1 = Fluxtower_NEE_JJAclean0[finitemask2]\n",
    "\n",
    "Huber_tot_JJA_S_NEE_slps=[]\n",
    "Huber_tot_JJA_S_NEE_ints=[]\n",
    "Huber_tot_JJA_S_NEE_R2=[]\n",
    "\n",
    "#try bootstrapping\n",
    "indx_list=list(range(0,len(S_NEE_JJAclean1)))\n",
    "for i in range(1,1001):\n",
    "    #sub selection of points\n",
    "    NEE_indx=np.random.choice(indx_list,size=len(S_NEE_JJAclean1))\n",
    "\n",
    "    Huber_model = linear_model.HuberRegressor(fit_intercept=True)\n",
    "    Huber_fit=Huber_model.fit((Fluxtower_NEE_JJAclean1[NEE_indx]).reshape(-1,1),S_NEE_JJAclean1[NEE_indx])\n",
    "    H_m=Huber_fit.coef_\n",
    "    H_c=Huber_fit.intercept_\n",
    "    x_accpt, y_accpt = Fluxtower_NEE_JJAclean1, S_NEE_JJAclean1\n",
    "    y_predict = H_m * x_accpt + H_c\n",
    "    H_R2=r2_score(y_accpt, y_predict)\n",
    "    Huber_tot_JJA_S_NEE_slps.append(H_m)\n",
    "    Huber_tot_JJA_S_NEE_ints.append(H_c)\n",
    "    Huber_tot_JJA_S_NEE_R2.append(H_R2)\n",
    "    \n",
    "y_predict = np.nanmean(Huber_tot_JJA_S_NEE_slps) * x_accpt + np.nanmean(Huber_tot_JJA_S_NEE_ints)\n",
    "Huber_JJA_S_NEE_R2=r2_score(y_accpt, y_predict)\n",
    "\n",
    "print('Updated SMUrF JJA slope: '+str(np.round(np.nanmean(Huber_tot_JJA_S_NEE_slps),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_JJA_S_NEE_slps),3)))\n",
    "print('Updated SMUrF JJA intercept: '+str(np.round(np.nanmean(Huber_tot_JJA_S_NEE_ints),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_JJA_S_NEE_ints),3)))\n",
    "\n",
    "print('Updated SMUrF JJA R^2: '+str(np.round(np.nanmean(Huber_JJA_S_NEE_R2),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SON: Doy 224 - 334 inclusive\n",
    "with np.errstate(invalid='ignore'):\n",
    "    SON_time=C_time_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    \n",
    "    #Borden Forest 2018:\n",
    "    Borden_GPPgf_SON=Borden_GEPgf[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_GPP_Borden_SON=C_GPP_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_GPP_Borden_SON=S_GPP_Borden[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    Borden_Rgf_SON=Borden_Rgf[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_Reco_Borden_SON=C_Reco_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_Reco_Borden_SON=S_Reco_Borden[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    Borden_NEEgf_SON=Borden_NEEgf[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    Borden_NEE_SON=Borden_NEE[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_NEE_Borden_SON=C_NEE_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_NEE_Borden_SON=S_NEE_Borden[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    #TP39 2018:\n",
    "    TP39_GPP_SON=TP39_GPP[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_GPP_TP39_SON=C_GPP_TP39_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_GPP_TP39_SON=S_GPP_TP39[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    TP39_R_SON=TP39_R[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_Reco_TP39_SON=C_Reco_TP39_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_Reco_TP39_SON=S_Reco_TP39[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    TP39_NEEgf_SON=TP39_NEEgf[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    TP39_NEE_SON=TP39_NEE[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_NEE_TP39_SON=C_NEE_TP39_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_NEE_TP39_SON=S_NEE_TP39[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    #TP39 2019:\n",
    "    TP39_2019_GPP_SON=TP39_2019_hrly_GPP[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_GPP_TP39_2019_SON=C_GPP_TP39_2019_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_GPP_TP39_2019_SON=S_GPP_TP39_2019[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    TP39_2019_R_SON=TP39_2019_hrly_R[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_Reco_TP39_2019_SON=C_Reco_TP39_2019_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_Reco_TP39_2019_SON=S_Reco_TP39_2019[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    TP39_2019_NEEgf_SON=TP39_2019_hrly_NEEgf[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    TP39_2019_NEE_SON=TP39_2019_hrly_NEE[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_NEE_TP39_2019_SON=C_NEE_TP39_2019_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_NEE_TP39_2019_SON=S_NEE_TP39_2019[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    #TPD 2018:\n",
    "    TPD_GPP_SON=TPD_GPP[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_GPP_TPD_SON=C_GPP_TPD_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_GPP_TPD_SON=S_GPP_TPD[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    TPD_R_SON=TPD_R[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_Reco_TPD_SON=C_Reco_TPD_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_Reco_TPD_SON=S_Reco_TPD[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    TPD_NEEgf_SON=TPD_NEEgf[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    TPD_NEE_SON=TPD_NEE[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_NEE_TPD_SON=C_NEE_TPD_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_NEE_TPD_SON=S_NEE_TPD[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    #TPD 2019:\n",
    "    TPD_2019_GPP_SON=TPD_2019_hrly_GPP[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_GPP_TPD_2019_SON=C_GPP_TPD_2019_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_GPP_TPD_2019_SON=S_GPP_TPD_2019[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    TPD_2019_R_SON=TPD_2019_hrly_R[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_Reco_TPD_2019_SON=C_Reco_TPD_2019_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_Reco_TPD_2019_SON=S_Reco_TPD_2019[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "\n",
    "    TPD_2019_NEEgf_SON=TPD_2019_hrly_NEEgf[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    TPD_2019_NEE_SON=TPD_2019_hrly_NEE[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    C_NEE_TPD_2019_SON=C_NEE_TPD_2019_array[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]\n",
    "    S_NEE_TPD_2019_SON=S_NEE_TPD_2019[(np.round(C_time_array,5)>=224) & (np.round(C_time_array,5)<335)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data over all fluxtowers:\n",
    "Fluxtower_GPP_SON_tot = np.concatenate([Borden_GPPgf_SON,TP39_GPP_SON,TPD_GPP_SON,TP39_2019_GPP_SON,TPD_2019_GPP_SON])\n",
    "C_GPP_SON_tot = np.concatenate([C_GPP_Borden_SON,C_GPP_TP39_SON,C_GPP_TPD_SON,C_GPP_TP39_2019_SON,C_GPP_TPD_2019_SON])\n",
    "S_GPP_SON_tot = np.concatenate([S_GPP_Borden_SON,S_GPP_TP39_SON,S_GPP_TPD_SON,S_GPP_TP39_2019_SON,S_GPP_TPD_2019_SON])\n",
    "\n",
    "Fluxtower_Reco_SON_tot = np.concatenate([Borden_Rgf_SON,TP39_R_SON,TPD_R_SON,TP39_2019_R_SON,TPD_2019_R_SON])\n",
    "C_Reco_SON_tot = np.concatenate([C_Reco_Borden_SON,C_Reco_TP39_SON,C_Reco_TPD_SON,C_Reco_TP39_2019_SON,C_Reco_TPD_2019_SON])\n",
    "S_Reco_SON_tot = np.concatenate([S_Reco_Borden_SON,S_Reco_TP39_SON,S_Reco_TPD_SON,S_Reco_TP39_2019_SON,S_Reco_TPD_2019_SON])\n",
    "\n",
    "Fluxtower_NEEgf_SON_tot = np.concatenate([Borden_NEEgf_SON,TP39_NEEgf_SON,TPD_NEEgf_SON,TP39_2019_NEEgf_SON,TPD_2019_NEEgf_SON])\n",
    "Fluxtower_NEE_SON_tot = np.concatenate([Borden_NEE_SON,TP39_NEE_SON,TPD_NEE_SON,TP39_2019_NEE_SON,TPD_2019_NEE_SON])\n",
    "C_NEE_SON_tot = np.concatenate([C_NEE_Borden_SON,C_NEE_TP39_SON,C_NEE_TPD_SON,C_NEE_TP39_2019_SON,C_NEE_TPD_2019_SON])\n",
    "S_NEE_SON_tot = np.concatenate([S_NEE_Borden_SON,S_NEE_TP39_SON,S_NEE_TPD_SON,S_NEE_TP39_2019_SON,S_NEE_TPD_2019_SON])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fit Original SMUrF data to fluxtower data using bootstrapped Huber fit\n",
    "          \n",
    "finitemask0 = np.isfinite(Fluxtower_NEE_SON_tot)\n",
    "Fluxtower_NEE_SONclean0 = Fluxtower_NEE_SON_tot[finitemask0]\n",
    "C_NEE_SONclean0 = C_NEE_SON_tot[finitemask0]\n",
    "\n",
    "finitemask2 = np.isfinite(C_NEE_SONclean0)\n",
    "C_NEE_SONclean1 = C_NEE_SONclean0[finitemask2]\n",
    "Fluxtower_NEE_SONclean1 = Fluxtower_NEE_SONclean0[finitemask2]\n",
    "\n",
    "Huber_tot_SON_C_NEE_slps=[]\n",
    "Huber_tot_SON_C_NEE_ints=[]\n",
    "Huber_tot_SON_C_NEE_R2=[]\n",
    "\n",
    "#try bootstrapping\n",
    "indx_list=list(range(0,len(C_NEE_SONclean1)))\n",
    "for i in range(1,1001):\n",
    "    #sub selection of points\n",
    "    NEE_indx=np.random.choice(indx_list,size=len(C_NEE_SONclean1))\n",
    "\n",
    "    Huber_model = linear_model.HuberRegressor(fit_intercept=True)\n",
    "    Huber_fit=Huber_model.fit((Fluxtower_NEE_SONclean1[NEE_indx]).reshape(-1,1),C_NEE_SONclean1[NEE_indx])\n",
    "    H_m=Huber_fit.coef_\n",
    "    H_c=Huber_fit.intercept_\n",
    "    x_accpt, y_accpt = Fluxtower_NEE_SONclean1, C_NEE_SONclean1\n",
    "    y_predict = H_m * x_accpt + H_c\n",
    "    H_R2=r2_score(y_accpt, y_predict)\n",
    "    Huber_tot_SON_C_NEE_slps.append(H_m)\n",
    "    Huber_tot_SON_C_NEE_ints.append(H_c)\n",
    "    Huber_tot_SON_C_NEE_R2.append(H_R2)\n",
    "    \n",
    "y_predict = np.nanmean(Huber_tot_SON_C_NEE_slps) * x_accpt + np.nanmean(Huber_tot_SON_C_NEE_ints)\n",
    "Huber_SON_C_NEE_R2=r2_score(y_accpt, y_predict)\n",
    "\n",
    "print('Original SMUrF SON slope: '+str(np.round(np.nanmean(Huber_tot_SON_C_NEE_slps),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_SON_C_NEE_slps),3)))\n",
    "print('Original SMUrF SON intercept: '+str(np.round(np.nanmean(Huber_tot_SON_C_NEE_ints),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_SON_C_NEE_ints),3)))\n",
    "\n",
    "print('Original SMUrF SON R^2: '+str(np.round(np.nanmean(Huber_SON_C_NEE_R2),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit updated SMUrF model to flux tower data in autumn using a bootstrapped Huber fit\n",
    "finitemask0 = np.isfinite(Fluxtower_NEE_SON_tot)\n",
    "Fluxtower_NEE_SONclean0 = Fluxtower_NEE_SON_tot[finitemask0]\n",
    "S_NEE_SONclean0 = S_NEE_SON_tot[finitemask0]\n",
    "\n",
    "finitemask2 = np.isfinite(S_NEE_SONclean0)\n",
    "S_NEE_SONclean1 = S_NEE_SONclean0[finitemask2]\n",
    "Fluxtower_NEE_SONclean1 = Fluxtower_NEE_SONclean0[finitemask2]\n",
    "\n",
    "Huber_tot_SON_S_NEE_slps=[]\n",
    "Huber_tot_SON_S_NEE_ints=[]\n",
    "Huber_tot_SON_S_NEE_R2=[]\n",
    "\n",
    "#try bootstrapping\n",
    "indx_list=list(range(0,len(S_NEE_SONclean1)))\n",
    "for i in range(1,1001):\n",
    "    #sub selection of points\n",
    "    NEE_indx=np.random.choice(indx_list,size=len(S_NEE_SONclean1))\n",
    "\n",
    "    Huber_model = linear_model.HuberRegressor(fit_intercept=True)\n",
    "    Huber_fit=Huber_model.fit((Fluxtower_NEE_SONclean1[NEE_indx]).reshape(-1,1),S_NEE_SONclean1[NEE_indx])\n",
    "    H_m=Huber_fit.coef_\n",
    "    H_c=Huber_fit.intercept_\n",
    "    x_accpt, y_accpt = Fluxtower_NEE_SONclean1, S_NEE_SONclean1\n",
    "    y_predict = H_m * x_accpt + H_c\n",
    "    H_R2=r2_score(y_accpt, y_predict)\n",
    "    Huber_tot_SON_S_NEE_slps.append(H_m)\n",
    "    Huber_tot_SON_S_NEE_ints.append(H_c)\n",
    "    Huber_tot_SON_S_NEE_R2.append(H_R2)\n",
    "\n",
    "y_predict = np.nanmean(Huber_tot_SON_S_NEE_slps) * x_accpt + np.nanmean(Huber_tot_SON_S_NEE_ints)\n",
    "Huber_SON_S_NEE_R2=r2_score(y_accpt, y_predict)\n",
    "\n",
    "print('Updated SMUrF SON slope: '+str(np.round(np.nanmean(Huber_tot_SON_S_NEE_slps),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_SON_S_NEE_slps),3)))\n",
    "print('Updated SMUrF SON intercept: '+str(np.round(np.nanmean(Huber_tot_SON_S_NEE_ints),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_SON_S_NEE_ints),3)))\n",
    "\n",
    "print('Updated SMUrF SON R^2: '+str(np.round(np.nanmean(Huber_SON_S_NEE_R2),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate winter data (DJF): Doy 224 - 334 inclusive\n",
    "with np.errstate(invalid='ignore'):\n",
    "    DJF_time=C_time_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    Borden_GPPgf_DJF=Borden_GEPgf[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_GPP_Borden_DJF=C_GPP_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_GPP_Borden_DJF=S_GPP_Borden[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    Borden_Rgf_DJF=Borden_Rgf[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_Reco_Borden_DJF=C_Reco_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_Reco_Borden_DJF=S_Reco_Borden[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    Borden_NEEgf_DJF=Borden_NEEgf[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    Borden_NEE_DJF=Borden_NEE[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_NEE_Borden_DJF=C_NEE_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_NEE_Borden_DJF=S_NEE_Borden[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    TP39_GPP_DJF=TP39_GPP[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_GPP_TP39_DJF=C_GPP_TP39_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_GPP_TP39_DJF=S_GPP_TP39[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    TP39_R_DJF=TP39_R[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_Reco_TP39_DJF=C_Reco_TP39_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_Reco_TP39_DJF=S_Reco_TP39[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    TP39_NEEgf_DJF=TP39_NEEgf[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    TP39_NEE_DJF=TP39_NEE[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_NEE_TP39_DJF=C_NEE_TP39_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_NEE_TP39_DJF=S_NEE_TP39[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    TP39_2019_GPP_DJF=TP39_2019_hrly_GPP[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_GPP_TP39_2019_DJF=C_GPP_TP39_2019_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_GPP_TP39_2019_DJF=S_GPP_TP39_2019[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    TP39_2019_R_DJF=TP39_2019_hrly_R[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_Reco_TP39_2019_DJF=C_Reco_TP39_2019_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_Reco_TP39_2019_DJF=S_Reco_TP39_2019[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    TP39_2019_NEEgf_DJF=TP39_2019_hrly_NEEgf[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    TP39_2019_NEE_DJF=TP39_2019_hrly_NEE[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_NEE_TP39_2019_DJF=C_NEE_TP39_2019_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_NEE_TP39_2019_DJF=S_NEE_TP39_2019[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    TPD_GPP_DJF=TPD_GPP[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_GPP_TPD_DJF=C_GPP_TPD_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_GPP_TPD_DJF=S_GPP_TPD[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    TPD_R_DJF=TPD_R[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_Reco_TPD_DJF=C_Reco_TPD_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_Reco_TPD_DJF=S_Reco_TPD[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    TPD_NEEgf_DJF=TPD_NEEgf[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    TPD_NEE_DJF=TPD_NEE[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_NEE_TPD_DJF=C_NEE_TPD_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_NEE_TPD_DJF=S_NEE_TPD[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    TPD_2019_GPP_DJF=TPD_2019_hrly_GPP[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_GPP_TPD_2019_DJF=C_GPP_TPD_2019_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_GPP_TPD_2019_DJF=S_GPP_TPD_2019[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    TPD_2019_R_DJF=TPD_2019_hrly_R[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_Reco_TPD_2019_DJF=C_Reco_TPD_2019_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_Reco_TPD_2019_DJF=S_Reco_TPD_2019[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "\n",
    "    TPD_2019_NEEgf_DJF=TPD_2019_hrly_NEEgf[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    TPD_2019_NEE_DJF=TPD_2019_hrly_NEE[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    C_NEE_TPD_2019_DJF=C_NEE_TPD_2019_array[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]\n",
    "    S_NEE_TPD_2019_DJF=S_NEE_TPD_2019[(np.round(C_time_array,5)>=335) | (np.round(C_time_array,5)<60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fluxtower_GPP_DJF_tot = np.concatenate([Borden_GPPgf_DJF,TP39_GPP_DJF,TPD_GPP_DJF,TP39_2019_GPP_DJF,TPD_2019_GPP_DJF])\n",
    "C_GPP_DJF_tot = np.concatenate([C_GPP_Borden_DJF,C_GPP_TP39_DJF,C_GPP_TPD_DJF,C_GPP_TP39_2019_DJF,C_GPP_TPD_2019_DJF])\n",
    "S_GPP_DJF_tot = np.concatenate([S_GPP_Borden_DJF,S_GPP_TP39_DJF,S_GPP_TPD_DJF,S_GPP_TP39_2019_DJF,S_GPP_TPD_2019_DJF])\n",
    "\n",
    "Fluxtower_Reco_DJF_tot = np.concatenate([Borden_Rgf_DJF,TP39_R_DJF,TPD_R_DJF,TP39_2019_R_DJF,TPD_2019_R_DJF])\n",
    "C_Reco_DJF_tot = np.concatenate([C_Reco_Borden_DJF,C_Reco_TP39_DJF,C_Reco_TPD_DJF,C_Reco_TP39_2019_DJF,C_Reco_TPD_2019_DJF])\n",
    "S_Reco_DJF_tot = np.concatenate([S_Reco_Borden_DJF,S_Reco_TP39_DJF,S_Reco_TPD_DJF,S_Reco_TP39_2019_DJF,S_Reco_TPD_2019_DJF])\n",
    "\n",
    "Fluxtower_NEEgf_DJF_tot = np.concatenate([Borden_NEEgf_DJF,TP39_NEEgf_DJF,TPD_NEEgf_DJF,TP39_2019_NEEgf_DJF,TPD_2019_NEEgf_DJF])\n",
    "Fluxtower_NEE_DJF_tot = np.concatenate([Borden_NEE_DJF,TP39_NEE_DJF,TPD_NEE_DJF,TP39_2019_NEE_DJF,TPD_2019_NEE_DJF])\n",
    "C_NEE_DJF_tot = np.concatenate([C_NEE_Borden_DJF,C_NEE_TP39_DJF,C_NEE_TPD_DJF,C_NEE_TP39_2019_DJF,C_NEE_TPD_2019_DJF])\n",
    "S_NEE_DJF_tot = np.concatenate([S_NEE_Borden_DJF,S_NEE_TP39_DJF,S_NEE_TPD_DJF,S_NEE_TP39_2019_DJF,S_NEE_TPD_2019_DJF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Original SMUrF data using bootstrapped Huber fit \n",
    "finitemask0 = np.isfinite(Fluxtower_NEE_DJF_tot)\n",
    "Fluxtower_NEE_DJFclean0 = Fluxtower_NEE_DJF_tot[finitemask0]\n",
    "C_NEE_DJFclean0 = C_NEE_DJF_tot[finitemask0]\n",
    "\n",
    "finitemask2 = np.isfinite(C_NEE_DJFclean0)\n",
    "C_NEE_DJFclean1 = C_NEE_DJFclean0[finitemask2]\n",
    "Fluxtower_NEE_DJFclean1 = Fluxtower_NEE_DJFclean0[finitemask2]\n",
    "\n",
    "Huber_tot_DJF_C_NEE_slps=[]\n",
    "Huber_tot_DJF_C_NEE_ints=[]\n",
    "Huber_tot_DJF_C_NEE_R2=[]\n",
    "\n",
    "#try bootstrapping\n",
    "indx_list=list(range(0,len(C_NEE_DJFclean1)))\n",
    "for i in range(1,1001):\n",
    "    #sub selection of points\n",
    "    NEE_indx=np.random.choice(indx_list,size=len(C_NEE_DJFclean1))\n",
    "\n",
    "    Huber_model = linear_model.HuberRegressor(fit_intercept=True)\n",
    "    Huber_fit=Huber_model.fit((Fluxtower_NEE_DJFclean1[NEE_indx]).reshape(-1,1),C_NEE_DJFclean1[NEE_indx])\n",
    "    H_m=Huber_fit.coef_\n",
    "    H_c=Huber_fit.intercept_\n",
    "    x_accpt, y_accpt = Fluxtower_NEE_DJFclean1, C_NEE_DJFclean1\n",
    "    y_predict = H_m * x_accpt + H_c\n",
    "    H_R2=r2_score(y_accpt, y_predict)\n",
    "    Huber_tot_DJF_C_NEE_slps.append(H_m)\n",
    "    Huber_tot_DJF_C_NEE_ints.append(H_c)\n",
    "    Huber_tot_DJF_C_NEE_R2.append(H_R2)\n",
    "    \n",
    "print('Original SMUrF DJF slope: '+str(np.round(np.nanmean(Huber_tot_DJF_C_NEE_slps),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_DJF_C_NEE_slps),3)))\n",
    "print('Original SMUrF DJF intercept: '+str(np.round(np.nanmean(Huber_tot_DJF_C_NEE_ints),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_DJF_C_NEE_ints),3)))\n",
    "\n",
    "y_predict = np.nanmean(Huber_tot_DJF_C_NEE_slps) * x_accpt + np.nanmean(Huber_tot_DJF_C_NEE_ints)\n",
    "Huber_DJF_C_NEE_R2=r2_score(y_accpt, y_predict)\n",
    "print('Original SMUrF DJF R^2: '+str(np.round(np.nanmean(Huber_DJF_C_NEE_R2),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fit SMUrF data to fluxtower data using bootstrapped Huber fit (with downscaling & fluxtower fix)\n",
    "          \n",
    "finitemask0 = np.isfinite(Fluxtower_NEE_DJF_tot)\n",
    "Fluxtower_NEE_DJFclean0 = Fluxtower_NEE_DJF_tot[finitemask0]\n",
    "S_NEE_DJFclean0 = S_NEE_DJF_tot[finitemask0]\n",
    "\n",
    "finitemask2 = np.isfinite(S_NEE_DJFclean0)\n",
    "S_NEE_DJFclean1 = S_NEE_DJFclean0[finitemask2]\n",
    "Fluxtower_NEE_DJFclean1 = Fluxtower_NEE_DJFclean0[finitemask2]\n",
    "\n",
    "Huber_tot_DJF_S_NEE_slps=[]\n",
    "Huber_tot_DJF_S_NEE_ints=[]\n",
    "Huber_tot_DJF_S_NEE_R2=[]\n",
    "\n",
    "#try bootstrapping\n",
    "indx_list=list(range(0,len(S_NEE_DJFclean1)))\n",
    "for i in range(1,1001):\n",
    "    #sub selection of points\n",
    "    NEE_indx=np.random.choice(indx_list,size=len(S_NEE_DJFclean1))\n",
    "\n",
    "    Huber_model = linear_model.HuberRegressor(fit_intercept=True)\n",
    "    Huber_fit=Huber_model.fit((Fluxtower_NEE_DJFclean1[NEE_indx]).reshape(-1,1),S_NEE_DJFclean1[NEE_indx])\n",
    "    H_m=Huber_fit.coef_\n",
    "    H_c=Huber_fit.intercept_\n",
    "    x_accpt, y_accpt = Fluxtower_NEE_DJFclean1, S_NEE_DJFclean1\n",
    "    y_predict = H_m * x_accpt + H_c\n",
    "    H_R2=r2_score(y_accpt, y_predict)\n",
    "    Huber_tot_DJF_S_NEE_slps.append(H_m)\n",
    "    Huber_tot_DJF_S_NEE_ints.append(H_c)\n",
    "    Huber_tot_DJF_S_NEE_R2.append(H_R2)\n",
    "    \n",
    "print('Updated SMUrF DJF slope: '+str(np.round(np.nanmean(Huber_tot_DJF_S_NEE_slps),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_DJF_S_NEE_slps),3)))\n",
    "print('Updated SMUrF DJF intercept: '+str(np.round(np.nanmean(Huber_tot_DJF_S_NEE_ints),3))+' +/- '+str(np.round(np.nanstd(Huber_tot_DJF_S_NEE_ints),3)))\n",
    "\n",
    "y_predict = np.nanmean(Huber_tot_DJF_S_NEE_slps) * x_accpt + np.nanmean(Huber_tot_DJF_S_NEE_ints)\n",
    "Huber_DJF_S_NEE_R2=r2_score(y_accpt, y_predict)\n",
    "print('Updated SMUrF DJF R^2: '+str(np.round(np.nanmean(Huber_DJF_S_NEE_R2),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with downscaling & fluxtower fix\n",
    "\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "plt.rc('font',size=21.5)\n",
    "fig, ax = plt.subplots(1,4,sharex=True,sharey=True,figsize=(24,6))\n",
    "ax[0].set_xlim(-69,25)\n",
    "ax[0].set_ylim(-69,25)\n",
    "\n",
    "ax[0].axvline(0,linestyle=(0, (3, 2, 1, 2, 1, 2)),c='k')\n",
    "ax[0].axhline(0,linestyle=(0, (3, 2, 1, 2, 1, 2)),c='k')\n",
    "ax[0].scatter(Fluxtower_NEE_MAM_tot,C_NEE_MAM_tot,s=5)\n",
    "ax[0].scatter(Fluxtower_NEE_MAM_tot,S_NEE_MAM_tot,s=5)\n",
    "ax[0].plot(line1_1,func2(line1_1,np.nanmean(Huber_tot_MAM_C_NEE_slps),np.nanmean(Huber_tot_MAM_C_NEE_ints)),linestyle='--',label=str(np.round(np.nanmean(Huber_tot_MAM_C_NEE_slps),2))+'$\\cdot$x+'+str(np.round(np.nanmean(Huber_tot_MAM_C_NEE_ints),2))+', R$^2$ = '+str(np.round(Huber_MAM_C_NEE_R2,2)),c='k', path_effects=[pe.Stroke(linewidth=5, foreground='#006BA4'), pe.Normal()])\n",
    "ax[0].plot(line1_1,func2(line1_1,np.nanmean(Huber_tot_MAM_S_NEE_slps),np.nanmean(Huber_tot_MAM_S_NEE_ints)),linestyle='-.',label=str(np.round(np.nanmean(Huber_tot_MAM_S_NEE_slps),2))+'$\\cdot$x+'+str(np.round(np.nanmean(Huber_tot_MAM_S_NEE_ints),2))+', R$^2$ = '+str(np.round(Huber_MAM_S_NEE_R2,2)),c='k', path_effects=[pe.Stroke(linewidth=5, foreground='#FF800E'), pe.Normal()])\n",
    "ax[0].plot(line1_1,line1_1,linestyle=':',c='k')\n",
    "\n",
    "ax[0].legend(loc='lower center')\n",
    "ax[0].set_title('Spring')\n",
    "\n",
    "ax[0].axvline(0,linestyle=(0, (3, 2, 1, 2, 1, 2)),c='k')\n",
    "ax[0].axhline(0,linestyle=(0, (3, 2, 1, 2, 1, 2)),c='k')\n",
    "ax[1].scatter(Fluxtower_NEE_JJA_tot,C_NEE_JJA_tot,s=5)\n",
    "ax[1].scatter(Fluxtower_NEE_JJA_tot,S_NEE_JJA_tot,s=5)\n",
    "ax[1].plot(line1_1,func2(line1_1,np.nanmean(Huber_tot_JJA_C_NEE_slps),np.nanmean(Huber_tot_JJA_C_NEE_ints)),linestyle='--',label=str(np.round(np.nanmean(Huber_tot_JJA_C_NEE_slps),2))+'$\\cdot$x+'+str(np.round(np.nanmean(Huber_tot_JJA_C_NEE_ints),2))+', R$^2$ = '+str(np.round(Huber_JJA_C_NEE_R2,2)),c='k', path_effects=[pe.Stroke(linewidth=5, foreground='#006BA4'), pe.Normal()])\n",
    "ax[1].plot(line1_1,func2(line1_1,np.nanmean(Huber_tot_JJA_S_NEE_slps),np.nanmean(Huber_tot_JJA_S_NEE_ints)),linestyle='-.',label=str(np.round(np.nanmean(Huber_tot_JJA_S_NEE_slps),2))+'$\\cdot$x+'+str(np.round(np.nanmean(Huber_tot_JJA_S_NEE_ints),2))+', R$^2$ = '+str(np.round(Huber_JJA_S_NEE_R2,2)),c='k', path_effects=[pe.Stroke(linewidth=5, foreground='#FF800E'), pe.Normal()])\n",
    "\n",
    "ax[1].plot(line1_1,line1_1,linestyle=':',c='k')\n",
    "ax[1].legend(loc='lower center')\n",
    "\n",
    "ax[1].set_title('Summer')\n",
    "\n",
    "ax[1].axvline(0,linestyle=(0, (3, 2, 1, 2, 1, 2)),c='k')\n",
    "ax[1].axhline(0,linestyle=(0, (3, 2, 1, 2, 1, 2)),c='k')\n",
    "ax[2].scatter(Fluxtower_NEE_SON_tot,C_NEE_SON_tot,s=5)\n",
    "ax[2].scatter(Fluxtower_NEE_SON_tot,S_NEE_SON_tot,s=5)\n",
    "ax[2].plot(line1_1,func2(line1_1,np.nanmean(Huber_tot_SON_C_NEE_slps),np.nanmean(Huber_tot_SON_C_NEE_ints)),linestyle='--',label=str(np.round(np.nanmean(Huber_tot_SON_C_NEE_slps),2))+'$\\cdot$x - '+str(np.round(-np.nanmean(Huber_tot_SON_C_NEE_ints),2))+', R$^2$ = '+str(np.round(Huber_SON_C_NEE_R2,2)),c='k', path_effects=[pe.Stroke(linewidth=5, foreground='#006BA4'), pe.Normal()])\n",
    "ax[2].plot(line1_1,func2(line1_1,np.nanmean(Huber_tot_SON_S_NEE_slps),np.nanmean(Huber_tot_SON_S_NEE_ints)),linestyle='-.',label=str(np.round(np.nanmean(Huber_tot_SON_S_NEE_slps),2))+'$\\cdot$x+'+str(np.round(np.nanmean(Huber_tot_SON_S_NEE_ints),2))+', R$^2$ = '+str(np.round(Huber_SON_S_NEE_R2,2)),c='k', path_effects=[pe.Stroke(linewidth=5, foreground='#FF800E'), pe.Normal()])\n",
    "\n",
    "ax[2].plot(line1_1,line1_1,linestyle=':',c='k')\n",
    "ax[2].legend(loc='lower center')\n",
    "ax[2].set_title('Autumn')\n",
    "\n",
    "ax[2].axvline(0,linestyle=(0, (3, 2, 1, 2, 1, 2)),c='k')\n",
    "ax[2].axhline(0,linestyle=(0, (3, 2, 1, 2, 1, 2)),c='k')\n",
    "ax[3].scatter(-100,-100,label='Original SMUrF',c='#006BA4')\n",
    "ax[3].scatter(-100,-100,label='Updated SMUrF',c='#FF800E')\n",
    "ax[3].scatter(Fluxtower_NEE_DJF_tot,C_NEE_DJF_tot,s=5)\n",
    "ax[3].scatter(Fluxtower_NEE_DJF_tot,S_NEE_DJF_tot,s=5)\n",
    "\n",
    "ax[3].axvline(0,linestyle=(0, (3, 2, 1, 2, 1, 2)),c='k')\n",
    "ax[3].axhline(0,linestyle=(0, (3, 2, 1, 2, 1, 2)),c='k')\n",
    "ax[3].plot(line1_1,line1_1,linestyle=':',c='k')\n",
    "ax[3].legend(loc='lower right',fontsize=23)\n",
    "ax[3].set_title('Winter')\n",
    "ax[0].set_ylabel('Modelled NEE ($\\mu$mol m$^{-2}$ s$^{-1}$)')\n",
    "\n",
    "ax[0].text(-67,17.5,'(e)',c='k',fontsize=26)\n",
    "ax[1].text(-67,17.5,'(f)',c='k',fontsize=26)\n",
    "ax[2].text(-67,17.5,'(g)',c='k',fontsize=26)\n",
    "ax[3].text(-67,17.5,'(h)',c='k',fontsize=26)\n",
    "\n",
    "fig.text(0.5, 0.01, 'Flux Tower NEE ($\\mu$mol m$^{-2}$ s$^{-1}$)', ha='center')\n",
    "fig.subplots_adjust(hspace=0,wspace=0)\n",
    "\n",
    "# *** UNCOMMENT TO SAVE FIGURE (CHANGE FILENAME) ***\n",
    "plt.savefig('Seasonal_Original_fixed_Updated_SMUrF_vs_fixed2_fluxtower_Huber_fit_NEE_0_lines_larger_font_cb_friendly_labelled.pdf',bbox_inches='tight')\n",
    "plt.savefig('Seasonal_Original_fixed_Updated_SMUrF_vs_fixed2_fluxtower_Huber_fit_NEE_0_lines_larger_font_cb_friendly_labelled.png',bbox_inches='tight')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
